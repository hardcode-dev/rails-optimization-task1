# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
1. Создал 3 тестовых файла на 1, 10 и 100 тысяч строк
2. Настроил метрику profilers/benchmark-ruby.rb чтобы она обрабатывала их поочередно и печатала результат
3. После каждой итерации сравнивал результат с предыдущим
3. Изначальные результаты были следующие:
    small finished in 0.04
    medium finished in 1.76
    на data_big метод завершился с ошибкой, large я даже не пробовал

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 

1. Настроил два профайлера ruby-prof-flat & ruby-prof-graph на прогон метода в файлом data-medium
2. Брал первую строчку отчета от ruby-prof-flat, изучал что ее вызывает в ruby-prof-graph, смотрел где можно оптимизировать
3. Если не получалось с первой стрчкой, смотрел на вторую
4. После первых двух итераций переключил профайлеры на data-large.txt, так как даже большой файл обрабатывался меньше секунды


## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof-graph 

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- #### какой отчёт показал главную точку роста
  ruby-prof-flat, Array#select занимал 45.44%	времени
- #### как вы решили её оптимизировать
  Решил использовать более оптимальную структуру данных для user, a sessions вообще не хранить, а сразу аггрегировать данные в user 
- #### как изменилась метрика
  small finished in 0.01
  medium finished in 0.09
  big finished in 0.78
  large finished in 33.21
- #### как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?
  Array#select исчез из метрики совсем, потому что больше не использовался
  Первая тройка в ruby-prof-flat выглядела так:
    21.92      1.080     0.258     0.000     0.822    15433  *Array#each                     
    17.37      0.432     0.205     0.000     0.227    84322   <Class::Date>#parse            
    8.46       0.100     0.100     0.000     0.000   168644   Regexp#match  

### Ваша находка №2
- #### какой отчёт показал главную точку роста
  ruby-prof-flat на огромном файле (data-large)
  Первая строчка отвечала за перебор строк в основном файле, вторая показалась мне перспективной
  17.37      0.432     0.205     0.000     0.227    84322   <Class::Date>#parse       
- #### как вы решили её оптимизировать
  парсинг дат тут вообще не нужен, iso8601 отлично сортируется как строка, никаких указаний на то что даты могут быть в разных форматах не было
  поэтому я заменил:
  "dates": user[:dates].map {|d| Date.parse(d)}.sort.reverse.map { |d| d.iso8601 }
  на:
  "dates": user[:dates].sort{|a,b| b <=> a }
- #### как изменилась метрика
  small finished in 0.0
  medium finished in 0.04
  big finished in 0.43
  large finished in 15.64 (23)
  Но тут я заметил, что при измерении benchmark отключал Garbage collector, при его включении большой файл стал обрабатываться за 23 секунды
- #### как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?
    <Class::Date>#parse исчезла из отчета
    33.69      0.399     0.237     0.000     0.161    15432   Array#each                     
    14.24      0.100     0.100     0.000     0.000   100001   String#split                   
    12.47      0.088     0.088     0.000     0.000   100001   Array#uniq                     
    8.43       0.079     0.059     0.000     0.020    30863   Array#sort  


### Ваша находка №3
- #### какой отчёт показал главную точку роста
  ruby-prof-flat на огромном файле (data-large)
  31.32     13.082     7.576     0.000     5.505   500001   Array#each  
- #### как вы решили её оптимизировать
  я решил использовать встроеный в руби CSV парсер и посмотреть что будет
- #### как изменилась метрика
  На удивление она изменилась в худшую сторону (23 -> 30)
  small finished in 0.01
  medium finished in 0.09
  big finished in 0.84
  large finished in 30.9
- #### как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?
  Я его не делал, откатил все обратно как было после шага 2

### Ваша находка №4
- #### какой отчёт показал главную точку роста
    ruby-prof-flat на огромном файле (data-large), 
    первая и вторая строчка это чтение строк и разбивка их на значения
    31.32     13.082     7.576     0.000     5.505   500001   Array#each                     
    14.75      3.569     3.569     0.000     0.000  3250941   String#split                   
    12.13      2.934     2.934     0.000     0.000  3250941   Array#uniq 
- #### как вы решили её оптимизировать
    я решил убрать uniq из основного цикла, он использовался при сборе дат.
- #### как изменилась метрика
    small finished in 0.00
    medium finished in 0.05
    big finished in 0.47
    large finished in 20.68
- #### как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?
    35.44     11.360     7.440     0.000     3.920        1   Array#each                     
    17.09      3.587     3.587     0.000     0.000  3250941   String#split                   
    8.59      2.369     1.804     0.000     0.566  1000001   Array#sort      

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с ошибки, до 20 секунд и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы я написал тесты, которые проверяют работу программы на файлах data_medium и data_big, а также проверяет корректность. Два теста должны защитить алгоритм от усложнения.

