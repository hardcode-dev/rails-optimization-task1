# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
- обрабатывать 200_000 строк не больше чем за 1,5 сек, если я уложусь в это время, то обработка всего файла уложится в бюджет

Асимптотика перед рефакторингом:
``` bash
"Processing time from file 1000 rows: 0.0177"
"Processing time from file 2000 rows: 0.0504"
"Processing time from file 4000 rows: 0.1516"
"Processing time from file 8000 rows: 0.504"
```

Асимптотика после первой итерации рефаторинга(формирование и группировка сессий по user_id):

```bash
"Processing time from file 1000 rows: 0.0078"
"Processing time from file 2000 rows: 0.0185"
"Processing time from file 4000 rows: 0.0358"
"Processing time from file 8000 rows: 0.0685"
```

Асимптотика после второй итерации(создание пользователей сразу при обходе строк)

```bash
"Processing time from file 1000 rows: 0.0084"
"Processing time from file 2000 rows: 0.0176"
"Processing time from file 4000 rows: 0.0372"
"Processing time from file 8000 rows: 0.0707"
```

Асимптотика после третьей итерации(оптимизация `collect_stat_from user`)

```bash
"Processing time from file 1000 rows: 0.0102"
"Processing time from file 2000 rows: 0.0234"
"Processing time from file 4000 rows: 0.0592"
"Processing time from file 8000 rows: 0.0849"
"Processing time from file 100000 rows: 1.0904"
"Processing time from file 1000000 rows: 41.2784" // полный отчет
```

Асимптотика финальная - видно что она почти линейная
```bash
"Processing time from file 1000 rows: 0.007"
"Processing time from file 2000 rows: 0.012"
"Processing time from file 4000 rows: 0.0232"
"Processing time from file 8000 rows: 0.0452"
"Processing time from file 100000 rows: 0.5962"
"Processing time from file 1000000 rows: 29.8777"
```

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 5 минут(внес изменеия -> запустил бенчмарк и профилировщик-> посмотрел результат)

Вот как я построил `feedback_loop`:
1. Создал Makefile с короткими алиасами для быстрого запуска тестов отчетов и бенчмарков
2. Запуск бенчмарков и профилировщика
3. Поиск точки роста
4. Внисение изменений
5. Проверка гипотизы
6. На каждой тиерации увеличивать данные на вход

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
1. Отчет graph html показал что главная точка роста является вызов `Array#select` программа проводит там 24% времени
2. Недостаток текущей реализации был в том что программа на каждом пользователи фильтровала массив обектов сессий
   Решение - группировать сессии пользователя по `user_id` при парсинге файла и потом достовать их уже по ключу
3. После этого при входящих данных в 8000 строк время работы уменьшилось с 0.504 до 0.0685, то есть программа ускорилась ~7 раз
4. После этого исправленная проблемма перестала быть точкой роста


### Ваша находка №2
1. Отчет callstack.htm показал что теперь основное время программа находится в `Array#each` вызовы которой происходит в нескольких местах: обход строк файла, обход пользователей для формирования объектов пользователей. Так как обход строк файла нам необходим 
2. Решил оптимизировать обход объектов пользователей - не делать повторный обход, а создавть пользователя сразу при обходе строк
3. Прирост метрики по бенчмаркам был незначительный 5%
4. По отчету точка роста изменилась - теперь программа большую часть времени стала проводить в методе `collect_stats_from_users` 34%
   Но я приблизился к бюджету - после оптимизации этой точки роста сборка отчета всех данных занимает 44 сек.

### Ваша находка №3
1. Отчет callstack.htm показал что теперь основное время программа находится в `collect_stats_from_users`
2. решил повторно не обходить массив обектов пользователя, а собирать по ним данные сразу во время парсинга
3. Время полног отчета хоть и уменьшилось до 41 сек, но большого прироста я не получил, по этому сделал откат назат 
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?

### Ваша находка №4
1. Еще раз сделал отчет callstack.htm с выключенным gc - отчет показа что есть точка рост в методе который собирает сессии пользователей, а также на инкрементирование прогресс бара
2. Оптимизация заключалась в следующем - добавлять сессии сразу в обект пользователя при обходе строк файла и отключить прогресс-бар при бенчмарках
3. Получил ощютимый прирост - формирование полного за 29.8777 сек.
4. После этого исправленная проблемма перестала быть точкой роста и я вышел на целевой бюджет - время формирования отчета за 30 сек


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *9,36 до 1.3274* и уложиться в заданный бюджет(полный отчет формируется за 29.8777).

*Какими ещё результами можете поделиться*
1. Пробовал запускать коллектинг данных по пользователям в несколько тредов(на кажый вызов `collect_stats_from_users(report, users_objects) do |user|` отдельный тред) - в моем случае это привело к регресу производительности с 37 сек. до 60
2. метод `size` работает гораздо быстрее чем `count` - удалось выйграть на большом колличестве данных 3 сек.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал perfomance тест, который запускает метод `work` 10  раз и проверяет на 200_000 строках что он исполняется не более 1,5 сек.

