# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Первым делом провел бенчмарки на меньшем объёме данных:

- 5000 строк - 0.53с.
- 10000 строк - 2.59с.
- 25000 строк - 20.36с.
- 50000 строк - 78.19с.
- 100000 строк - 275.12c.

Рост нелинейный, наша задача привести эти метрики к линейному виду. Главная метрика - обработка исходного файла, она должна укладываться в 30 сек.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 5-10 сек.

Вот как я построил `feedback_loop`:
 - Нарезал данные на куски меньшего объёма
 - После каждого изменения - запуск бенчмарка и анализ

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
 - rbspy
 - ruby-prof
 - stackprof

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Попробовал сделать быструю поверхностную оценку через rbspy. Получил информацию о том, что 99% времени уходит на метод work. Информации маловато.
- Посмотрел flat report при помощи ruby-prof. 89% времени уходит на Array#select
- Посмотрел на структуру получаемых данных и изучил код. В данных, которые мы получаем явно задана последовательность между юзером и сессиями, которую мы никак не учитываем и обрабатываем входящие данные, как случайную последовательность
- Пользуясь знаниями о структуре данных можем сделать вывод о том, что нам не нужно отдельно сохранять юзеров и сессии и потом сопоставлять друг с другом, достаточно грамотно обработать входные данные
- Избавившись от структур users и sessions и оставив только user_objects, в которую мы сразу же пишем входные данные получили следующие результаты:
- 5000 строк - 0.13с.
- 10000 строк - 0.24с.
- 25000 строк - 0.52c.
- 50000 строк - 1.10c.
- 100000 строк - 2.62c.
- Нам удалось перейти к линейному росту. Главная точка роста устранена.

### Ваша находка №2
- Тем же способом через flat report определил следующую точку роста Array#all? Заодно попробовал graph и callstack, увидел аналогичную картину
- Проверка if uniqueBrowsers.all? { |b| b != browser } не оптимальна, заменил на unless uniqueBrowsers.include?(browser)
- Получили следующий результат
- 5000 строк - 0.08с.
- 10000 строк - 0.18с.
- 25000 строк - 0.43c.
- 50000 строк - 0.91c.
- 100000 строк - 2.15c.
- Показатели улучшились, главная точка устранена.

### Ваша находка №3
- С помощью stackprof speedscope определил, что более 50% времени занимает #collect_stats_from_users
- Вместо того, чтобы вызывать метод несколько раз собираем всю статистику за один проход по массиву юзеров + убираем лишние вызовы map
- Получили следующий результат
- 5000 строк - 0.07с.
- 10000 строк - 0.15с.
- 25000 строк - 0.37c.
- 50000 строк - 0.8c.
- 100000 строк - 1.97c.
- Показатели улучшились. Главная точка ещё не устранена.

### Ваша находка №4
- С помощью ruby-prof graph report определил, что большую часть времени collect_stats_from_users проводит в Date.parse
- Избавился от вызова Date.parse
- Получили следующий результат
- 5000 строк - 0.05с.
- 10000 строк - 0.10с.
- 25000 строк - 0.27c.
- 50000 строк - 0.57c.
- 100000 строк - 1.60c.
- Показатели улучшились. Главная точка устранена.

### Ваша находка №5
- С помощью ruby-prof flat report определил, что главные проблемы остались в String#split, Array#each и Array#map.
- Изменил парсинг строк -> зафиксировал улучшения, Изменил подсчёт браузеров -> зафиксировал улучшения.
- Получили следующий результат
- 5000 строк - 0.04с.
- 10000 строк - 0.07с.
- 25000 строк - 0.16c.
- 50000 строк - 0.4c.
- 100000 строк - 1.03c.
- Показатели улучшились, но главные точки всё те же.

### Ваша находка №6
- Поскольку большую часть времени занимают итерации, нам нужно их количество снизить до возможного минимума, т.е. собрать все данные за один проход.
- Изменил алгоритм так, чтобы все данные собирались за один проход.
- Получили следующий результат
- 5000 строк - 0.03с.
- 10000 строк - 0.07с.
- 25000 строк - 0.14c.
- 50000 строк - 0.27c.
- 100000 строк - 0.54c.
- Показатели улучшились, ускорение почти в 2 раза. Исходный файл обработали за 24.56с. Написанный ранее тест на производительность пройден. Задача выполнена.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *никогда* до *24.56с* и уложиться в заданный бюджет.
Что можно отметить:
1. Нужно обращать внимание не только на код, но и на структуру обрабатываемых данных. Работая с изначально отсортированными/структурированными данными можно значительно улучшить производительность просто изменив обработку входных данных, не затрагивая основную логику приложения.
2. Самым удобным способом определить главную точку роста мне показался flat-report. Если точка на поверхности, можно получить отклик и сразу ее найти буквально за 5 секунд. Если же проблема более глубокая, flat-report'a уже может быть недостаточно. Но в качестве первой итерации, пожалуй, самый удачный вариант.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написан тест на обработку исходного файла за 30 сек.
