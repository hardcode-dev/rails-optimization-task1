# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время выполнения метода work

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 20-30 сек, без учета времени на рефакторинг

Вот как я построил `feedback_loop`:
- Профилирование, для поиска главной точки роста
- Внесение изменений
- Прогон тестов
- Замер времени Benchmark'ом
- Коммит

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался
- Benchmark для беглого анализа асимптотики показал, что при увеличении данных в 2 раза, время работы программы увеличивалось примерно в 4 раза, отключение сборщика мусора на время работы не влияло
- ruby-prof
- stackprof

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Все отчеты показали, что главная точка роста в строке
  `user_sessions = sessions.select { |session| session['user_id'] == user['id'] }`
- Sessions из массива переделал в хэш и формировал user_sessions на этапе чтения файла, а также uniqueBrowsers и totalSessions, так как при их формировании тоже требовался массив sessions
- На файле в 8000 строк время сократилось с 3.06 сек до 0.28, 16000 строк с 12.19 до 0.6, а также при увеличении данных в 2 раза, время работы программы стало увеличиваться примерно в 2 раза
- Исправленная проблема перестала быть главной точкой роста

### Ваша находка №2
- Все отчеты показали, что главная точка роста в строке
  `sessions[:uniqueBrowsers] += [browser] if sessions[:uniqueBrowsers].all? { |b| b != browser }`
- Также переделал uniqueBrowsers на хэш
- На файле в 8000 строк время сократилось с 0.28 сек до 0.23, 16000 строк с 0.6 до 0.46
- Исправленная проблема перестала быть главной точкой роста

### Ваша находка №3
- stackprof показал главную точку роста в строке с парсингом даты
- К счастью, данные приходили в нужном формате, по этому убрал работу с датой
- На файле в 8000 строк время сократилось с 0.28 сек до 0.15, 16000 строк с 0.6 до 0.3
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?

### Ваша находка №4
- stackprof показал главную точку роста в методе parse_session и parse_user
- убрал в медотах ненужную запись в переменную (что дало прирост скорости) и изменил ключи со строк для символов (что не особо дало прирост скорости)
- На файле в 8000 строк время сократилось с 0.15 сек до 0.13, 16000 строк с 0.3 до 0.27, после этого я решил увеличить объем данных для замера, чтобы результаты не воспринимались погрешностью: 50000 строк - 1.02 сек, 100000 строк - 2.7
- Исправленная проблема перестала быть главной точкой роста

### Ваша находка №5
- Отчеты показали главную точку роста в методах map во время collect_stats_from_users
- Собрал все collect_stats_from_users в один и записал вызовы map в переменные, плюс не докрутил в предыдущий раз split и убрал его из parse методов
- На файле в 50000 строк - с 1.02 сек до 0.82 сек., 100000 строк - 2.7 до 2.13
- Исправленная проблема перестала быть главной точкой роста

### Ваша находка №6
- RubyProf показал главную точку роста в методах each (во всех примерно равное время)
- Объединил все в один проход путем сортировки массива строк, тем самым получив для начала возможность разобраться с сессиями, а потом с пользователями и генерацией отчета по ним
- На файле в 50000 строк - с 1.02 сек до 0.56 сек., 100000 строк - 2.7 до 1.18. Также накануне пробовал скрипт на файле в 1000000 строк и результат был 87 сек против 14.83 теперь
- Исправленная проблема перестала быть главной точкой роста

### Ваша находка №7
- RubyProf показал главную точку роста в методе to_json
- Применил oj
- На файле в 50000 строк - с 0.56 сек. до 0.44 сек., 100000 строк - 1.18 до 0.9, 200000 строк - 1.9
- Исправленная проблема перестала быть главной точкой роста

## Результаты
Все последующие точки роста я не смог придумать, как можно переделать и выиграть по времени. В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с времени, дождаться которое пока никому не удавалось, до 46.61 сек на ruby 2.6.3 и 42.55 сек на ruby 3.0.1. К сожалению, уложиться в заданный бюджет не удалось, свой результат считаю вполне достойным.

*Какими ещё результами можете поделиться*
Во время 3го захода не очень разобразолся с отчетами, видел, что много времени уходит на Array#map, и решил что все map по чуть-чуть сжирают, переписал логику для totalTime и longestSession на подсчет при чтении файла и записи этих данных в хэш, окрыленный успехом предыдущих попыток, обнаружил, что время исполнения даже чуть-чуть увеличилось. Ушел заново копать отчеты.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавлен перфоманс тест в файле perfomance-spec.rb

