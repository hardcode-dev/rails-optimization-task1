# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 30 sec для файла в 1_000_000 строк(data_large.txt)

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Вот как я построил `feedback_loop`:

- я замерил время работы программы при разном количестве строк(10_000, 20_000 ... 100_000) и определил наиболее подходящий для меня файл.
Также по точкам я построил график роста времени от количества строк и обнаружил логарифмическую асимптотику.
Данные по координатам были такие:

- 10_000 - 1.6 sec
- 20_000 - 6 sec
- 30_000 - 13 sec
- 40_000 - 25 sec
- 50_000 - 45 sec
- 60_000 - 80 sec

data_large.txt не удалось дождаться, но по аимптотике понятно, что там огромное значение.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался 

- rbspy
- ruby-prof (все виды отчетов из лекции)
- stackprof (все виды отчетов из леции)

На данном этапе для построения feedback-loop использовал файл с 30_000 строк.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- отчеты показывали одинаковые результаты, мне больше понравился ruby-prof(callstack). </br>
Главная точка роста была в методе `select`, там был перебор для каждого юзера всех сессий.
- Идея оптимизации была в том, чтобы создать структуру данных, которую можно было бы наполнить для юзеров при первом парсинге строк из файла. Тем самым избавился от N количества `select` по всему массиву `sessions`.
- c 13 sec до 2 sec на 30_000 строк
- метод `select` выпал из топа и на его место встал метод `each`

### Ваша находка №2
- ruby-prof(callstack)
- точка роста была в методе `each`(их на самом деле было несколько и все внутри `work`) </br>
В основном оптимизация в том, чтобы при парсинге строк сделать все что нужно, без создания массивов users и sessions и дальнейшего прохождения по ним `each`.
- с 2 sec до 0.41 sec на 30_000 строк
- `each` выпал из топа и на его место встал `collect_stats_from_users` и внутри него большую часть времени проводили в `map`

### Ваша находка №3
- ruby-prof(callstack)
- `collect_stats_from_users` вызывался 7 раз с блоком, в в кажом из которых был как минимум один `map`
Идея оптимизации была в том, чтобы сделать только один `map` по всем сессиям юзера и за один проход посчитать все необходимые данные.
- c 0.41 sec до 0.1 sec на 30_000 и 56 sec на data_large.txt
- `collect_stats_from_users` остался в топе, но внутри него из топа исчез `map`, а его место занял `Date.parse`

### Ваша находка №4
- ruby-prof(callstack)
- в `collect_stats_from_users` парсили дату, чтобы потом отсортировать ее в массиве.
основная идея была в том, чтобы избавится вообще от парсинга даты, потому что по сути сам объект класса Date там не нужен нигде больше
Заменил парсинг даты на sort_by.
- время с 30_000 стало совсем незначительным, а время на data_large.txt уложилось в бюджет(29 sec)
- sort_by встал на место Date.parse, но при этом время работы программы значительно уменьшилось.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с времени значительно больше бюджета(на data_large.txt не дождался метрики) и уложиться в заданный бюджет(29 sec).

Сделал еще несколько улучшений, например если юзер хоть раз пользовался 'INTERNET EXPLORER', то он точне не мог пользоваться только 'CHROME', значит и проверять не имеет смысла.

Дважды делали split одной и той же строки, сделал так, чтобы делали только единажды.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан performance spec с помощью rspec на данных в 10_000 строк.

