# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

1. Для начала я выбрал оптимальное кол-во строк для обработки - `10 000 строк`;
2. При помощи гема `benchmark/ips`, замерил за какое кол-во времени выполняется метод work ~ `6 секунд`;
3. Измерял метрику на ноутбуке HP с процессором `Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz`.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за `6 секунд`

Вот как я построил `feedback_loop`:

1. Собрал метрику;
2. Работа с профилировщиком и анализ данных;
3. Поиск решений для оптимизации.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:

1. Gem `ruby-prof`
2. Gem `benchmark/ips`
3. [fast-ruby](https://github.com/fastruby/fast-ruby)

Вот какие проблемы удалось найти и решить

### Находка №1
- Первым делом я избавился от лишнего на мой взгляд класса User, решив заменить его на хэш с ключом - user_id
и значением - хеш с данными по сессиям пользователя.

### Находка №2
- При помощи отчёта из `ruby-prof` я обнаружил первые две на мой взгляд главные точки. Первая - 99 строка перебор пользователей и 101 строка - выбор сессий для пользователя;
- Я принял решение полностью изменить логику и заполнять все данные в процессе перебора строк текстового файла;
- Производительность возрасла более чем на ~ 80%.

### Находка №3
- После переработки логики с построением хеша пользователей, я решил избавиться от метода `collect_stats_from_users`
в котором производились действия по преобразованию данных по сессиям пользователя;
- Проходя по всем пользователям и оптимизируя преобразование данных из сессиий у меня получилось улучшить производительность ~ 10%.

### Находка №4
- При текущей реализации файл `data_large.txt` обрабатывался уже менее `30 скунд`.
- Далее я воспользовавался материалом из [fast-ruby](https://github.com/fastruby/fast-ruby). 
Ускорил преобразования общего хеша `report` в json при помощи гема `oj`. Воспользовался гемом `sorted-set` для сбора уникальных браузеров.
И внес еще много мелких правок, которые уже не оказывали сильного влияния на производительность.

## Результаты
В результате проделанной оптимизации мне наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с того, что обработка файла `data_large.txt` длилась вечность, до ~ `11,60 секунд`  и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы я воспользовался гемом RSpec.
При помощи RSpec был написан тест для проверки скорости и тест для проверки корректности данных после обработки.
