# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Необходимо исправить проблему. Для заданного объема файла мы должны выполнить обработку за не более 30 секунд.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: кол-во секунд за которое должна выполниться программа

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 2 минуты.

Вот как я построил `feedback_loop`:
Для начала, я отрезал куски от файла по 5_000, 10_000, 20_000, 40_000 строк.
Далее с помощью benchmark я замерил скорость их выполнения.
Получил 0.62, 2.5, 15 и 60 секунд выполнения соответсвенно.
Которая говорит о нелинейной зависимости.
По примерным подсчетам, полный объем информации в 3_250_940 строк должен обработаться за 527312 секунд (т.е. 6 суток)
Если получить линейную зависимость, то при заданом условии обработка 10_000 строк должна выполнится за 0.075 секунды.

Поэтому в feedback_loop я буду использовать именно это кол-во строк.
Для того чтобы не испортить и так не очень-то "хорошую" картину, я написал тест на текущую метрику.


ПС: Что интересно, отключение GC привело к увеличению времени обработки 20_000 строк на 5 секунд.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался профилировщиками rbspy и ruby-prof.
Самым информативным оказался ruby-prof в режиме Graph. Я больше всего им и пользовался.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- ruby-prof в режиме Graph показал, что самым большее кол-во времени занимает работа метода Array#each.
Но их три в методе main, поэтому я решил сделать алиасы и посмотреть какой из них больше всего работает.
Выяснилось, что больше всего времени занимает третий Array#each, а в нем Array#select (sessions.select).
Это место я и начал оптимизировать.
- Решил заменить select на два действия:
  Сессии обернул в класс;
  Получил Hash из массива сессий с группировкой по user_id, а далее начал получать массив сессий просто по ключу.
- Оказалось успешным решение решением, метрика уменьшилась в 5 раз (0.55 секунд), а зависимость осталась нелинейной.
- Используя ruby-prof и stackprof, я выяснил, что эта проблема перестала быть главной точкой роста,
  и теперь главная точка роста находится на первом Array#each.

### Ваша находка №2
- Теперь главная точка роста на первом Array#each, вот ее и буду оптимизировать.
- Атрибуты обернул в класс, и объединение массивов заменил на push нового элемента в массив
- Оказалось успешным решение решением, метрика уменьшилась еще в два раз (0.27 секунд), а зависимость осталась нелинейной.
- Используя ruby-prof и stackprof, я выяснил, что эта проблема перестала быть главной точкой роста,
  и теперь главная точка роста находится на втором Array#each, а точнее на методе Array#all?.


### Ваша находка №3
- Теперь главная точка роста на Array#all?, вот ее и буду оптимизировать.
- uniqueBrowsers сделал экземпляром класса Set, и в момент инициализации я передаю массив всех браузеров.
- Оказалось успешным решение решением, метрика уменьшилась еще немного (0.21 секунд), а зависимость осталась нелинейной.
- Используя ruby-prof и stackprof, я выяснил, что эта проблема перестала быть главной точкой роста,
  и теперь главная точка роста находится на Array#map.

### Ваша находка №4
- Теперь главная точка роста на Array#map второй используется в Object#collect_stats_from_users, вот ее и буду оптимизировать.
- Для того чтобы понять какой из методов Array#map самый долгий, я решил опять сделать алиасы на него.
- И ruby-prof показал, что главная точка роста это Array#each в Object#collect_stats_from_users, а самым долгим Array#map был тот, который работал с датами. Поэтому пока займусь работой Array#each

### Ваша находка №5
- Буду оптимизировать Array#each в Object#collect_stats_from_users
- Немного переработал работу с хешем статистики, убрал collect_stats_from_users, уменьшил кол-во map'ов.
- Метрика совсем немного улучшилась
- Главная точка роста перешла на String#split, который вызывается из Object#parse_user и Object#parse_session

### Ваша находка №6
- Буду оптимизировать String#split
- Убрал лишнии String#split в Object#parse_user и Object#parse_session
- Метрика немного улучшилась (0.15 секунд)
- Главная точка роста перешла на Array#each, созданный на шаге №5

### Ваша находка №7
- Буду оптимизировать Array#each
- Убрал сбор статистики в класс UserStats
- Метрика немного улучшилась (0.14 секунд)
- Главная точка роста перешла на Array#map, который занимается парсингом даты

### Ваша находка №8
- Буду оптимизировать парсинг даты
- Удалил парсинг, тк в нашем случае он не нужен
- Метрика немного улучшилась (0.1 секунд)
- Главной точки роста уже нет, а так около 9 точек с 8%, но побольше была с Array#each для создания массивов user и sessions

### Ваша находка №9
- Буду оптимизировать Array#each
- Переписал с группировкой и использованием map
- Метрика немного улучшилась (0.09 секунд)
- Более выделялась точка роста с обработкой json

### Ваша находка №10
- Буду оптимизировать json
- Добавил гем oj
- Метрика немного улучшилась (0.08 секунд)
- Более выделялась String#split, он нужен (Попробовал CSV.parse, но время только увеличилось).

### Ваша находка №11
- Дальше оптимизация зашла в тупик. Оставновился на 34.85 секундах за весь
- Последний отчет лежит в директории ruby_prof_reports/graph.html

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 6 суток до 35 секунд.

Для более удобной работы сделал отдельные скрипты запуска бенчмарка, профилировщика и тестов.


## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы я написал два теста:
- Выполнение теста не более заданного бюджета
- Увеличение времени выполнения
