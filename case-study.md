# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Цель

На основе данных профилировщиков, оптимизировать исходный код приложения вплоть до 30 секунд выполняния в режиме *WALL_TIME* без отключения сборщика мусора.

## Задачи

Профилировать приложение всеми возможными инструментами, которые мы прошли за первую неделю.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *временная*

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- rbspy и ruby-prof показали, что именно Array#select является главной точкой роста, которую в последующем я и устранил используя банальных хэш.

### Ваша находка №2
- опробовав все профайлеры, решил на данном отчете остановиться на ruby-prof#Graph, он показал, что следующая точка роста была Array#all? - 37.26%, который как временное решение я решил крайне просто, методом uniq по всем браузерам с count. Но уже понимаю, что с кодом все крайне печально и очень манит батчем многое переписать, но приходится делать это степ бай степ

### Ваша находка 3
- следующая остановка была на `Array#each` 23.70% и многократном вызове `collect_stats_from_users`. Также смутил block.call, так как на сколько я помню yield куда быстрее

### Ваша находка 4
- следующий рефакторинг, решил сделать с `Array#map` 15.39%	опять же, который учавствует в формировании конечного результата в методе `collect_stats_from_users`. Решил вынести подсчет некоторых вещей на несколько шагов выше, а именно в парсинг `CSV` файла, попутно заменю `File.read(file_name).split("\n")` с последующим `Array.each` по нему, на `CSV#foreach`, И заметил непонятные лишние `split`-ы, тоже решил их прибрать.

### Ваша находка 5
- рефакторинг на 4 шаге оказался достаточно большим и не заметил момент, что в формировании json-а используется зачем-то `<Class::Date>#parse`, который после последнего рефакторинга занимает 7.51%. Убираю, так как он просто там не нужен, как и последующий `Array#map` для `iso8601`

### Ваша находка 6
-  решил затестить сколько займет времени парсинг всего файла и миллиона строк. Весь файл занял `43.46`, а миллион строк `12.12`. В принципе, осталось несколько очевидных для меня решений, которые можно улучшить. ruby-prof, показывает, значительное часть времени занимает метод `Array#count` 4.60%, их можно также убрать вынеся большую часть логики в парсинг CSV. Первым делом вынес подсчет количества юзеров. Дальше решил разобраться с количеством уникальных браузеров и количеством сессий. Попутно запуская между каждым изменением профайлер.

### Ваша находка 7
- запустив ещё раз ruby-prof, понял, что оказался в той ситуации, когда `Array#count` и `Array#map`, отъедают все ещё слишком много времени в %, взглянул на код и понял, что в отчетах данные по ключу `allBrowsers` сохраняются каким-то неймоверным способом. Правлю!
P.S. после правки, понял, что влезаю в мой бэнчмарк, но думаю не останавливаться, есть ещё пару моментов, которые хочу проверить.

### Ваша находка 7
- решил попробовать заменить `#to_json`, на oj но почему-то как оказалось, на производительности оно осбо не повлияло, чему я был немного удивлен. Может я что-то не так делал ¯\_(ツ)_/¯ 

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

На самом деле, научился тому, что давно хотел познать. Это читать флэймграф и в целом профилировать приложение. Так как инструменты были совершенно новыми для меня, пришлось потратить на них немного больше времени чем ожидал. В целом, уже есть несколько моментов которые я учел для себя и планирую попробовать оптимизировать 1 достаточно длинную джобу у нас на проекте.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
Написал простенький `performance_benchmark_spec`, который должен выполняться быстрее чем за 30 секунд.
