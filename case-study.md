# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *Время выполнения программы*

Для начала используя ассимптотку я сделал предположение о том, сколько будет работать программа на больших данных.
Для этого я написал простой тест, который запускает программу, передавая ей в качестве аргумента нужный файл

### Результаты:
* 7500 строк ~ 0.870 s
* 25000 строк ~ 12.152 s
* 50000 строк ~ 54.638 s
* 100000 строк ~ 221.902 s

Исходя из этих результатов можно сделать вывод что 3250940 строк обработается примерно за 3-4 дня. Оптимизировать действительно нужно.

Бюджет метрики - 30 секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *пару скунд*

Вот как я построил `feedback_loop`:
1. Написал performance тест для защиты от деградации производительности
2. Написал програмки для запуска различных профилировщиков.
3. Подключил Guard чтобы не тратить время на ручной запуск тестов

Сам `feedback_loop`:
1. запуск профилировщиков и выявление наиболее жирных мест.
1.1. Рефакторинг, если плохо понятно в каком именно месте программы узкое место
1.2. Запуск тестов
1.2. Goto 1
2. Внесение необходимых изменений
3. Запуск тестов
3.1. Откат изменений если тесты не прошли
4. Если метрика не соответствует бюджету - Goto 1

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *ruby-prof & stackprof*

Вот какие проблемы удалось найти и решить

### Находка №1
- Воспользовался `RubyProf`. Flat показал что наибольшее время занимает `Array#select`. Graph - показал что дольше всего работает `Array#each`, в котором вызывается `Array#select`. `Callstack` также указывает на `Array#each` -> `Array#select`. Да и callgrind это подтвердил.
- Я решил сделать группировку сессий по user_id. Для того чтобы обращаться по ключу, а не пробегать каждый раз весь массив
- Метрика стала лучше примерно в 15 раз
- Проблема с главной точкой роста исправлена. Можно закоммититься

### Находка №2
- Теперь главная точка роста `Array#each`. Проблема в том, что абсолютно не понятно который это each. Нужен рефакторинг, а именно вынос в методы. Первый же вынос в метод показал главную точку роста - это парсинг файла.
- для начала я решил попробавать не читать весь файл и затем разбивать строку на массив по '\n', а использовать `File.readlines(filename)`
- Метрика практически не изменилась
- Профилировщики показали незначительное улучшение парсинга файла, но главная точка роста не изменилась. Тем не менее хоти и не большое но улучшение, можно коммитить.

### Находка №3
- Главная точка роста та же. При этом из этого метода слишком часто вызывается split, что также показывает rubyprof отчет.
- split достаточно вызвать один раз для каждой строки, а не два раза как сейчас
- Метрика практически не изменилась
- Отчет профилировщика опять показал небольшое улучшение. Но главная точка роста не изменилась.

### Находка №4
- Главная точка роста та же. Попробовал использовать класс CSV для парсинга файла, но отчеты изменились до неузнаваемости. Стало гораздо больше внутрибиблиотечных вызовов, а также ухудшилась метрика. Откатился.
- Затем решил использовать `Array#push`, вместо конкатенации.
- Метрика стала лучше. Performance test стал проходить за 350 ms для 18500 записий (вместо 12500 прежде)
- Отчет профилировщика показал другую точку роста - метод `collect_stats_from_users`

### Находка №5
- Главная точка роста `Array#each` в методе `collect_stats_from_users`. Данный метод вызывется 7 раз, и 7 раз перебираются все найденные пользователи в файле. Метод принимает 3 параметра: итоговый отчет, массив, пользователей и блок. Отличия - только в блоках.
- Зачем делать 7 раз то, что можно сделать за один. Просто объединил все блоки в один.
- Метрика стала лучше. Performance test стал проходить за 350 ms для 20000 записий (вместо 18500 прежде)
- Отчет профилировщика показал другую точку роста - `Array#each` -> `Array#all?`

### Находка №6
- Главная точка роста `Array#each` -> `Array#all?` присутствует только в одном месте программы - при подсчете кол-ва уникальных браузеров.
- Заменил перебор в цикле с условием на `map().uniq`
- Метрика стала лучше. Performance test стал проходить за 350 ms для 26500 записий (вместо 20000 прежде). Неожиданно
- Отчет профилировщика показал другую точку роста - снова метод `collect_stats_from_users`, но теперь наибольшее время в нем занимает `Array#map`

### Находка №7
- Главная точка роста метод `collect_stats_from_users` -> `Array#map`. В этом методе мы 6 раз мапаем сессию для разных нужд. Это перебор.
- Уменьшил вдвое количество мапов сессий пользователя. А также убрал ненужный `Date#parse`, т.к. строки также прекрасно сортируются, и дата лежит уже в нужном формате.
- Метрика стала лучше. Performance test стал проходить за 350 ms для 47000 записий (вместо 26500 прежде). Вах
- Несмотря на разительное увеличение производительности, главная точка роста осталась таже)

### Находка №8
- Главная точка роста метод `collect_stats_from_users` -> `Array#map`. Скорее всего лучшим решеним будет агрегация нужной информации во время парсинга.
- Оптимизация заключалось в выносе всей агрегации данных для отчета из метода `collect_stats_from_users` в парсинг. Т.е. сразу суммируем и храним все значения при парсинге.
- Метрика уложилась в бюджет. Performance test стал проходить за 350 ms для 65000 записий (вместо 47000 прежде). Отчет на полных данных сформировался за ~ 25 секунд
- Главная точка роста переехала в метод `parse_file`.

### Находка №9
- Так как метрика уложилась в бюджет, оптимизацию проводить больше не нужно. Но вот рефакторинга не хватает, код немного "пахнет"
- Метрика стала даже несного лучше (наверное из-за того что убрал вызов блока). Теперь разбор 70000 строк укладывается в 350 ms.
- Весь файл теперь разбирается за чуть больше 24 секунд. (С отключеным GC - за 15 секунд)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы (если мерить по файлу со 100к записей) с 221 секунд, 0,55 сек и уложиться в заданный бюджет.

Тест на правильность нужно доработать. Он не проверяет корректность значения "Только хром".

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы я написал тест на производительноть, который замеряет время выполнения программы на заданном количестве данных. Таким образом при дольнейших внесениях изменений, мы защитимся от регрессии производительности.
