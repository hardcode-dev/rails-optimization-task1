# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

замер времени между началом и концом работы на семпле в 50000 строк, далее на `data_large.txt`
Замеряла просто вычитая время конца из времени начала. Но опять же, большую часть времени смотрела прогресс в отчёте `ruby-prof`

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построила (мой неидеальный) `feedback_loop`:

- подобрала sample , на котором программа выполняется за обозримое время (5-50 секунд)
- после того, как программа стала выполняться за 90 секунд, проверяла на большом объёме данных

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовалась `ruby-prof` (как основным инструментом).
Сначала в режиме flat, потом быстро перешла на `RubyProf::WALL_TIME`, в принципе этого хватило. Также пробовала stackprof.

Вот какие проблемы удалось найти и решить:

### Ваша находка №1
- какой отчёт показал главную точку роста
ruby-prof: call tree
```
7.59% (7.80%) Array#all? [8464 calls, 10000 total] 
```
на sample (10000)
5.62 => 4.8

Заменила `uniqueBrowsers` на `Set` (но основная оптимизация кмк за счёт убирания прохода по всем браузерам, можно было и `Array.include`)

не сильно изменилось, но это была бесячая вещь...

### Ваша находка №2

В принципе сразу тоже:

```
86.97% (89.02%) Array#select [1536 calls, 1536 total] 
```

Нужно перестать проходиться по всем сессиям при обработке каждого пользователя.
Например, сделать ассоциативный массив с ключом user и значением - массив сессий.

А то и сразу собрать массив объектов `user` и его `sessions`.
Также сразу можем собрать `uniqueBrowsers` и общее кол-во сессий, чтобы потом не считать.

Также, `report['allBrowsers']` - это и есть  `uniqueBrowsers` , только нужно отстортировать и заджойнить.

Теперь результат -  0.22433002200000374

Результат профилировщика изменился ))

Сразу попробуем на больших данных.

72.75583059900032 - уже можно жить

### Находка 3

Теперь много времени тратим на `Object#collect_stats_from_users`, в т.ч. `map`.
В принципе можно просто разок пройтись по юзерам и сессиям и собрать `report['usersStats'][user_key]`

52.531833646999985 сек

### Находка 4

Вижу, что много времени тратится на парсинг даты, закэширую даты в Hash, вдруг повезёт.

39.95 - неплохо!

### Находка 5

Теперь `parse_sessions` много времени отнимает.
Сделаем без массива:

```ruby
_, user_id, session_id, browser, time, date = cols.split(',')
  {
    'user_id' => user_id,
    'session_id' => session_id,
    'browser' => browser,
    'time' => time,
    'date' => date,
  }
```

Ещё пыталась парсить с `csv` , но это было супердолго!

36.794 - ну окей, пусть будет

### Находка 6

И всё-таки не нравится мне эти `file_lines`...
прочитаем построчно `File.readlines`

36.31289 - в пределах погрешности? Ну пусть будет

### Время проверить без профилировщика и с GC!

26.353673987

Okay.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с "невозможно дождаться выполнения на большом объёме данных, 5,6 с на 50000 строк" и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы:

- написан минимальный тест на performance, проверяющий время
