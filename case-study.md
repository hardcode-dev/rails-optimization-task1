# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему и оптимизировать эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы будем использовать простую метрику - **время обработки файла**

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Выстраиваем работу и Feedback-Loop

* Метод для обработки файлов `work` и тест для него находятся в одном файле - что не удобно. Вынес тест в отдельный файл. Для удобства запуска вызов метода также сделал в отдельном файле
* Добавил `Gemfile` с необходимыми `gem`'ами, зафиксируем версию `ruby`
* Добавил параметр с именем файла `filename` в метод  `work`
* Не забыл про `.gitignore`
* Исходный файл имеет более 3х миллионов строк `$ wc -l tmp/data_large.txt # 3250940 tmp/data_large.txt` и обрабатывается неприлично долго, поэтому сделаем файлы поменьше - такие, на которых программа отрабатывает достаточно быстро - это позволит вам выстроить фидбек-луп. Я сделал так:
```ruby
range = [1000, 2000, 4000, 8000, 10_000, 16_000, 20_000]
range.each do |n|
  `head -n #{n} tmp/data_large.txt > tmp/data_#{n}.txt`
end
# =>
# tmp/data_1000.txt
# tmp/data_2000.txt
# tmp/data_4000.txt
# ...
```
* Сделал небольшой скрипт с `Benchmark.realtime` для подсчета времени обработки полученных выше файлов. Получилось следующее:
```
$ ruby benchmark/benchmark_realtime.rb
1000 Finish in 0.03
2000 Finish in 0.08
4000 Finish in 0.27
8000 Finish in 0.94
10000 Finish in 1.43
16000 Finish in 3.37
20000 Finish in 5.14
в секундах
```
Видно, что асимптотика роста времени работы в зависимости от объёма входных данных не линейна. Намного поигравшись в excel стало понятно, что асимптотика скорее всего полиномиальная и больше O(N^2)

Интересно оценить сколько программа будет работать на полном объеме данных. Если все же предположить что у имеющегося алгоритма полиномиальный рост в N^2, то получится, что 3.2 миллиона строк будут обрабатываться не меньше чем 10_240_000 секунд, что примерно 120 дней - как-то многовато =)

### Защита от регрессии производительности
Для защиты от регрессии производительности воспользуемся гемом `rspec-benchmark`
В качестве опорной точки возьмем файл в 10_000 строк и напишем тест на метрику время обработки не более чем 2 сек
```ruby
expect {
  work('tmp/data_10000.txt')
}.to perform_under(2000).ms.warmup(2).times.sample(3).times
```

###  Профилирование
При профилировании лучше выключать  `GC`  (он может вносить непредсказуемые замедления в рандомные части программы). Поэтому, в метод `work` добавим параметр `disable_gc`.

Для профилирования я выбрал gem `ruby-prof`, который строит удобные html и txt отчеты. При запуске обработки через `ruby-prof` отключаем `GC` и строим отчеты в форматах `FlatPrinter`, `GraphHtmlPrinter` и `CallStackPrinter`

### Feedback-Loop
В итоге у меня получился эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений менее чем за 10 сек.

Скрипты для удобства можно добавить в `Makefile`:
```
.PHONY: test
test:
  ruby test/task_1_test.rb
  rspec test/performance_spec.rb
bench:
  ruby benchmark/benchmark_realtime.rb
  ruby benchmark/ruby-prof.rb
```

## Вникаем в детали системы, чтобы найти главные точки роста

### Находка №1
После анализы отчетов `ruby-prof` стало понятно, что главная точка раста программы - это вызовы `Array#select`, который занимал 85% времени обработки.
Этот метод в вызывался в одном месте, но, чтобы убедиться, я вынес этот вызов в отдельный метод и повторил профилирование.

Проблема была в том, что для каждого пользователя выбирались его сессии из всего списка
Применив мемоизацию (сохраняем сессии пользователя в хеше user) время обработки существенно сократилось и асимптотика приблизилась к линейной

```
$ ruby benchmark/benchmark_realtime.rb
1000 Finish in 0.02
2000 Finish in 0.03
4000 Finish in 0.07
8000 Finish in 0.15
10000 Finish in 0.2
16000 Finish in 0.31
20000 Finish in 0.42
```

Тесты проходят, ситуация в профилировщике `ruby-prof` изменилась - теперь можно обновить тест для защиты от регрессии производительности и сделать commit.

### Находка №2
В новых отчетах профилировщика `ruby-prof` видно, что теперь главная точка роста - `Array#all?` - Подсчёт количества уникальных браузеров
```
Total: 0.585065
Array#all? 35%
10000 Finish in 0.16
20000 Finish in 0.38
40000 Finish in 0.88
80000 Finish in 1.89
160000 Finish in 3.64
```
Оптимизируем подсчет применив `Set`, попутно поменяем allBrowsers на использование этого множества
```
Total: 0.395011
Array#all? - ушел из топа нагрузки
10000 Finish in 0.12
20000 Finish in 0.29
40000 Finish in 0.7
80000 Finish in 1.52
160000 Finish in 2.99
```

Прирост производительности достигнут. Тесты проходят, ситуация в профилировщике `ruby-prof` изменилась - теперь можно обновить тест для защиты от регрессии производительности и сделать commit.

### Находка №3
Устал смотреть на хеши со строками с хеш-рокетами - рефакторим!
Особого прироста нет, но глаз радуется.

### Находка №4
По аналогии с предыдущими находками продолжаем: Смотрим отчеты, понимаем что главная точка роста - многократные вызовы `Array#each` и `collect_stats_from_users`. Оптимизируем!

### Находка №5
Главная точка роста - многократные вызовы `Array#map` в `collect_stats_from_users`

\+ Заметил, что очень много ест `Date#parse`. В исходном файле даты в формате iso8601. Этот формат хорош тем, что если применить сортировку к строкам, результат будет такой же, как и при сортировки дат. Поэтому парсинг дат можно убрать =)

Убрал лишние `Array#map` - получил прирост почти в 2а раза

### Находка №6
Главная точка роста - многократные вызовы `String#split`

### Находка №7
После серии рефакторинга файл `data_large.txt` обрабатывается примерно 30 сек - что укладывается в **бюджет** выбранной метрики.
Одной из главных точек роста является формирование JSON. Решил попробовать библиотеку `oj` и не прогадал - получил почти 20% прироста

## Результаты  
В результате проделанной оптимизации удалось до обработать файл с данными.  
Удалось улучшить метрику системы с "бесконечности" до 25 секунд и уложиться в заданный бюджет. Кстати, если отключить GC программа выполняется за 12 секунд, но лучше этого не делать, чтобы избежать утечек памяти.

Заметил, что существующий тест на корректность не проверяет `alwaysUsedChrome`
Также обратил внимание на то, что браузеры и даты сессий по каждому пользователю и в тесте, и в исходной программе не уникальны - нужно обратиться к заказчику.

Метод  `collect_stats_from_users` сейчас занимает 40% времени. Этот метод можно оптимизировать, и посчитать статистические данные при первом проходе по строкам файла, но прирост производительности не окупит вложенных усилий (вспоминаем принцип Парето)

**P.S.**

Все-таки не удержался и переписал скрипт с нуля, с прицелом на то, чтобы получать все данные за один проход - получилось 17 сек, но это уже совсем другая история.

Интересно, сколько времени займет формирование файла, если обработку вынести в БД?...
Очень актуально, если исходный txt файл - это и есть выгрузка из БД.
