# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 

Тестовый файл с данными в 135Мб должен укладываться в 30 секунд


## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 
на первом этапе, до оптимизаций я решил использовать тестовый файл размером 15000 который выполняется за 6 секунд что позволит быстро видеть результат и нивелирует влияние прогрева программы


## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался 

- запуск на различных обьемах данных чтобы понять зависимость времени работы от размера файла
- запуск на большой обьеме данных и профилировка rbspy
- запуск на небольшом обьеме с ruby-prof и qcachegrind

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- запуск на разных обьемах показал, что рост следующий 
1000 строк:  0.062604s
10_000 строк: 2.321406s
20_000 строк: 15.514703s
Зависимость явно хуже чем линейная, было решено попробовать rbspy - чем дольше времени работы тем лучше rbspy покажет проблемное место с экспоненциальным ростом
Но толку с этого не оказалось - все в c-function

### Ваша находка №2
- ruby-prof показал что основная проблема в Array.select
- как оказалось у нас всего 1 селект user_sessions = sessions.select
- так как сильно менять код вокруг не хочется, а по памяти у нас проблем нет я решил создать хэш для быстрого поиска по массиву

  hashed_sessions = sessions.group_by{ |session| session['user_id'] }
  и обновил поиск на 
  user_sessions = hashed_sessions[user['id']]
  
- 15_000 строк стали отрабатывать за 0.9с - что в 7 раз быстрее
- в отчете профилировщика основная проблема переехала в Array:each

### Ваша находка №3
- я увеличил тестовый обьем до 100_000 строк так как 15 ставли выполнятся слишком быстро
- ruby-prof Callgrind показал что проблема в Array.each вызываемом из work дальнейший анализ вкладки SourceCode показал что проблема тут 
  file_lines.each do |line|
    cols = line.split(',')
    users = users + [parse_user(line)] if cols[0] == 'user'
    sessions = sessions + [parse_session(line)] if cols[0] == 'session'
  end
- из отчета также видно что split, parse_user & parse_session занимают мало времени 
- дабы убедиться что проблема именно тут я сравнил время выполнения исходной программы и варианта где после этого array.each стоит return - время 14 и 17с подтвердило мою правоту - точка роста именно тут  
- методом комментирования я нашел что основная проблема в строке
sessions = sessions + [parse_session(line)] if cols[0] == 'session'
- так как я неопытный пользователь рубипрофа я решил проверить все ли я правильно понимаю и убрал парсинг сесии и юзера, время не изменилось
из чего я сделал вывод что проблема в том что парсинг каждой строки приводил к созданию минимум 3 новых обьектов типа массив и изменил код так
  file_lines.each do |line|
    cols = line.split(',')
    users << parse_user(line) if cols[0] == 'user'
    sessions << parse_session(line) if cols[0] == 'session'
  end
Это снизило время парсинга 100_000 строкового файла с 17 до 4c  
Я увеличил размер до 200_000 и рубипроф снова показал что проблема все еще в Array.each
Но теперь он вызывался из Enumerable#all? 169251/169256 но их два, в бой пошел GraphHtmlPrinter который показал что all вызывается из each и что collect_stats_from_users нипричем
Отлично, проблема найдена 
  uniqueBrowsers = []
  sessions.each do |session|
    browser = session['browser']
    uniqueBrowsers += [browser] if uniqueBrowsers.all? { |b| b != browser }
  end
Уникальные браузеры? Тут я придумал 2 способа как сделать быстрее но не знал какой лучше поэтому сделал бенчмарк 

  z1 = Benchmark.realtime { 20.times{ sessions.select{|session| session['browser'] }.uniq } }
  puts "test1 #{z1}"
  z2 =  Benchmark.realtime { 20.times {sessions.uniq{|session| session['browser']}.map{|session| session['browser']}} }
  puts "test2: #{z2}"

Оказалось что второй вариант намного быстрее 

test1 16.18229799999972
test2: 4.273353999999927

Потом я заметил что дальше массив не нужен так что я заменил код сразу на расчет каунт что оказалось еще быстрее
test2: 1.3805479999991803

Но в целом это особо не помогло, значит точку роста определили неправильно. 
По стате все еще 
array.each 90.58% total & 	50.90% self

Я вынес содержимое each в collect_stats_from_users в отдельную функцию fill_report_from_user там оказалось 39% времени но маленький селф
и тогда я переделал создание user_objects на collect 
 users_objects = users.collect do |user|
    attributes = user
    user_sessions = hashed_sessions[user['id']] || []

    User.new(attributes: attributes, sessions: user_sessions)
  end
тут заодно и ушло большое создание лишних массивов в сумме

Особо глобальных проблем не осталось, остались чуть поменьше, например DateParse 21%

Кстати большой файл уже парсится за 2м17с
  

### Ваша находка №4

Так как мы теперь можем распарсить большой файл за адекватное время я загнал его весь в рубипроф
22.53% <Class::Date>#parse смотрим stackoverflow меняем на strptime снизили до 5% 
Тут же сразу заметил map-map связки и map-any тоже присутствуют в ruby-prof - убираем 
map+max => max_by
map + sum => sum

И пока смотрел upcase постоянно вызывается на браузере хотя без апкейза не используется, всего 4% но заодно перенесу вызов

Так я вышел из двух минут, но последний шаг это явно было потешить самолюбие, надо искать реальные точки роста а не потакать своим желаниям


### Ваша находка №5

Hash#merge 12.73%
Меняем  report['usersStats'][user_key] = report['usersStats'][user_key].merge(block.call(user))
на report['usersStats'][user_key].merge!(block.call(user))

Результат
real	1m39.051s



### Ваша находка №6

По-прежнему RubyProf::GraphHtmlPrinter.new(result)
Object#fill_report_from_user 23.87%
Разбиваем все на методы и перезапускаем профилировку
Все размазалось +- равномерно по 7-8% никаких очевидных улучшений не видно
Прогон с отключенным GC дал прирост в 2 раза. Что пока непонятно но судя по всему генерируем много лишних обьектов, попробуем сократаить

Пойдем собирать по мелочи
- rubocop 
- fasterer
- time.to_i делается дважды - можно перенести в парсинг
- имя и фамилия отдельно ен используются - можно сразу положить в хэш вместе 
- age вообще не нужен - убираем
- постоянно дергать report['usersStats'][user_key] не нужно можно сперва сформировать хэш  usersStats а потом положить егов  родительский хэш

real	1m21.784s


### Ваша находка №7

По-прежнему RubyProf::GraphHtmlPrinter.new(result)
Очень много each и map полагнаю это потому то collect_stats_from_users вызывается 7 раз и мы 7 раз бегаем по циклу
убираем

  collect_stats_from_users(stats, users_objects) do |user|
    {
        'sessionsCount' => user.sessions.count,  # Собираем количество сессий по пользователям
        'totalTime' => user.sessions.sum { |s| s[:time] }.to_s + ' min.', # Собираем количество времени по пользователям
        'longestSession' => user.sessions.max_by { |s| s[:time] }[:time].to_s + ' min.', # Выбираем самую длинную сессию пользователя
        'browsers' => user.sessions.map { |s| s[:browser] }.sort.join(', '),  # Браузеры пользователя через запятую
        'usedIE' => user.sessions.any? { |b| b[:browser] =~ /INTERNET EXPLORER/}, # Хоть раз использовал IE?
        'alwaysUsedChrome' => user.sessions.all? { |b| b[:browser] =~ /CHROME/ },  # Всегда использовал только Chrome?
        'dates' => user.sessions.map { |s| Date.strptime(s[:date], '%Y-%m-%d') }.sort.reverse.map(&:iso8601)  # Даты сессий через запятую в обратном порядке в формате iso8601
    }
  end
Ну и так как нам теперь нет нужды делать merge 6 раз меняем сразу на присваивание того что возвращает блок collect_stats_from_users
real	1m14.442s
И сюда же в рамках борьбы с нвоыми обьектами для GC 

  unique_browsers = SortedSet.new(sessions.map { |s| s[:browser] })
  report['uniqueBrowsersCount'] = unique_browsers.count
  report['allBrowsers'] = unique_browsers.to_a.join(',')

real	1m5.162s  
  


### Ваша находка №8

Что у нас осталось в топе
to_json 13.39%
strptime 11.46%
User.new 8.33%
split 21.27%
map  6.90%
fill_report_from_user 6.08%
Array.each 6.30%

Заменил json либу на oj

real	0m58.699s


  

### Ваша находка №9

<Class::Date>#strptime 9.11%
При этом вроде как у нас и так нужный формат дат да еще и он таков что можно прямо по строкам сортировать

real	0m47.241s


### Ваша находка №10
Изменил фидбек луп на более быстрый - Becnhmark.ips файла из 1000 строк

Object#process_line	 41.21% / 7.24% self

Переписал на вот такую конструкцию 

  if cols[0] == 'session'
    sessions << parse_session(line)
  elsif cols[0] == 'user'
    users << parse_user(line)
  end
  
  Ничего не поменялось и тут я увидел что сплит делается дважды - тут и потом еще раз внутри, видели бы вы мое лицо, щас видимо и GC попроще станет

real	0m35.498s

### Ваша находка №11
Так как обьект work все езе большой я решил разбить его на более мелкие части
и по пути заметил что указанный ниже код не нужен

users_objects = users.collect do |user|
  attributes = user
  user_sessions = hashed_sessions[user[:id]] || []

  User.new(attributes: attributes, sessions: user_sessions)
end

Потому что все что нам ниже по коду нужно это получить отсюда сессии, так можно сразу и запросить их из hashed_sessions[user[:id]]


Дает небольшой прирост 
real	0m34.806s


### Ваша находка №12

17.27%	Object#fill_uniq_browser_stats и много времени в new - идея с SortedSet видимо была продумана не до конца, меняем на

unique_browsers = sessions.uniq{|s| s[:browser]}.map { |s| s[:browser] }
  
real	0m32.291s


### Ваша находка №13


56.30% (13.87% self) - Array.each line
- 41.89% (11.29% self)  process_line  - куда ужиматься дальше неясно

39.49% (6.44%) get_user_stats - вот тут поработаем поплотнее разобьем все на минифункции
- browses_sorted 8.01
- sorted_dates 5.84%
- hashed sessios 5.62%


Я был чертовски близок к тому чтобы ужать последние пары секунд но каждая новая мс давалась с боем. 
В итоге я решил что имеет смысл оптимизировать получение сессия по пользователям чтобы жэффективнее считать, 
потом решил считать часть сразу при парсинге, стало еще хуже и я понял что либо парсить либо считать сразу и решил считать сразу
Итог
real	0m21.361s
user	0m20.332s
sys	0m0.977s	
Надеюсь это не будет засчитано за переписывание


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с бесконечности до 22с и уложиться в заданный бюджет.



## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы 
я напсиал простенький тест проверяющий попадание в бюджет

