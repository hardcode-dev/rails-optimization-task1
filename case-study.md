# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.


## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*
1. Профилирование(поиск точки роста ruby-prof )
2. Изменение кода

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof

Вот какие проблемы удалось найти и решить

### Ваша находка №1

при использовании профилировщика ruby-prof, было выявлено, что Array#select точка роста

    %self      total      self      wait     child     calls  name 
    94.87     32.046    32.046     0.000     0.000     4982   Array#select

было решено select заменить на group_by по user_id,

    %self      total      self      wait     child     calls  name 
    0.00      0.006     0.000     0.000     0.006        1   Enumerable#group_by     

как видно из результатов это дало большой прирост по времени и group_by вызывается всего 1 раз в отличие от select

### Ваша находка №2
точка роста Array#each

    Было
    %self      total      self      wait     child     calls  name     
    56.70      1.566     0.931     0.000     0.635       11   Array#each

    Стало
    %self      total      self      wait     child     calls  name
    28.40     18.825     6.174     0.000    12.651   500003  *Array#each        

Произвел рефакторинг кода, переписал использование метода collect_stats_from_users.
Узким местом в блоках Array.each мне кажется было склеивание данных в массив, заменил 
склеивание массивов, добавлением элементов в уже существуюший массив

Более не вижу, что можно было бы еще оптимизировать.
По результатам Benchmark

    Файл с 32509-ю строками выполняется со скоростью 0.09450699994340539s
    Файл data_large.txt с 3250940 выполняется со скоростью 13.540022000204772s - с выключенным GC
    Файл data_large.txt с 3250940 выполняется со скоростью 24.074387999949977 - 26.51794799999334s - с включенным GC
С включенным GC уложился в бюждет 30 секунд