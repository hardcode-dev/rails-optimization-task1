# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время работы программы.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений.

Вот как я построил `feedback_loop`: создал несколько файлов с разными входными данными, чтобы профилирование длилось в среднем 5 - 20 с. Написал тест на производительность

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался rbspy, ruby_prof, stack_prof

Вот какие проблемы удалось найти и решить
(Сначала отрефакторил - разбил методы на более мелкие)

### находка №1
- rbspy показал, что самый жирный процесс в <c function> - unknown, ruby_prof_graph зашел больше всех и показал на Array#select
- т.к. долго выполнялся перебор большого массива, я решил сгруппировать его по user_id и обращаться по этому ключу
- обработка 10т строк уменьшилась с 9.7 с до 0.8 с
- исправленная проблема перестала быть главной точкой роста

### находка №2
- ruby_prof_graph показал в парсинге строк главную точку роста (но рядом были еще с чуть меньшим значением)
- увидел, что метод split используется лишние разы, удалил.
- обработка 15т строк уменьшилась с 1.3 с до 1.26 с
- исправленная проблема НЕ перестала быть главной точкой роста

### находка №3
- то же, что и в №2
- предварительно ключи user и session сконвертировал в символы и далее сгруппировал массивы по этим ключам, также убрал добавление каждого распарсенного элемента в массив (заменил на map) 
- обработка 15т строк уменьшилась с 1.26 с до 0.92 с
- исправленная проблема перестала быть главной точкой роста

Тем временем асимптотика приблизилась к линейной, это значит, что 100т строк должны обрабатываться меньше чем за секунду.
Сейчас 7.7 с.

### находка №4
- ruby_prof_graph показал главную точку роста в методе collect_stats_from_users
- для начала изменил присваивание в хэш на метод merge! 
- обработка 100т строк уменьшилась с 7.7 с до 5.9 с
- исправленная проблема НЕ перестала быть главной точкой роста

### находка №5
- то же, что и в №4
- перенес collect_stats_from_users в родительский метод в один итератор, дабы один раз вычислялись ключ юзера и его сессии и хэш присваивался за 1 раз, а также один раз выбирались браузеры и время
- обработка 100т строк уменьшилась с 5.9 с до 5.3 с
- исправленная проблема ПОЧТИ перестала быть главной точкой роста

### находка №6
- ruby_prof_graph показал главную точку роста в методе count_uniq_browsers
- переписал с использованием uniq
- обработка 100т строк уменьшилась с 5.3 с до 3.1 с
- исправленная проблема перестала быть главной точкой роста

### находка №7
- ruby_prof_graph показал главную точку роста в методе user_stats в итераторе each
- убрал ненужную конвертацию даты туда сюда
- обработка 100т строк уменьшилась с 3.1 с до 1.7 с
- исправленная проблема НЕ перестала быть главной точкой роста

Немного завис и подкорректировал код вслепую (иногда давало прирост, иногда наоборот).
...
## Результаты
Перепробовал много чего, попробовал на работе запустить, был неприятно удивлен, 
что исходный файл парсится за 37 с на рабочем компе (дома на ноуте около 50).
Уперся в методы типа sort, all?, split и не придумал как их переписать, они вроде нужны.
В общем решил избавиться от класса User и попробовал засунуть всё в хэш с ключом по имени пользователя и,
ruby_prof_graph показал, что миллион строк стали обрабатываться 4,7 с вместо 5,7, а исходный файл с 37 с ускорился до 20 с.
Это ли не чудо?!

В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с бесконечности до ~20 секунд и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан performance-тест
который падает если 10т строк обрабатываются более 30 мс (это с запасом).

