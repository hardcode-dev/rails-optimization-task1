# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики

Основная метрика - время отбработки файла.

Размер исходного файла 3 250 940 строк. 

Так как выполнение текущей реализация занимает слишком много времени, была проведена оценка времени выполнения.

Finish 100 lines in 0.001 seconds

Finish 1000 lines in 0.028 seconds

Finish 10000 lines in 1.767 seconds

Finish 20000 lines in 10.839 seconds

Уже на 50 000 текущая реализация занимает слишком много времени.

В качестве начальной метрики была взято время обработки 20 000 записей.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 20 секунд

За это время выполнялся замер метрики и анализ корректности работы на тестовых данных

Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof c отчетами flat, graph, callstack 

Вот какие проблемы удалось найти и решить

### Находка №1
- отчет graph показал что больше всего времени выполняется метод Array#select
- я нашел что он вызывается по каждому пользователя для всего массива сессий и предварительно сгруппировал сессиипо пользователю  
- метрика уменьшилась примерно 13 раз
- исправленная проблема перестала быть главной точкой роста

- надо обновить метрику. Теперь метрикой будет обработка 100 000 строк. Сейчас это 10 секунд. 

### Находка №2
- отчет flat показал что основное время уходит в методе Array#each
- надо разделил код на именованные функции
- не увеличилась
- как изменился отчёт профилировщика - отчет callstack показал, что половина времени уходит на чтение и создание массивов users и sessions

### Находка №3
- отчет graph показал что метод split выполняется два раза для каждой строчки
- убрал лишние вызовы
- метрика уменьшилась на 1 секунду
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? нет

### Находка №4
- отчет callstack показал, что половина времени уходит на чтение создание массивов users и sessions
- переписал построчную обработка групповой
- метрика уменьшилась в 4 раза
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? Да

### Находка №5
- отчет callstack показал, что 40% времени уходит на сбор статистики по пользователям
- собрал сбор статистики по одному пользователю в обдин вызов
- метрика улучшилась на 20% 
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? Да 

### Находка №6
- отчет callstack показал, что 42% времени уходит на сбор уникальных браузеров
- переписал сбор уникальных браузеров
- метрика улучшилась на 25% 
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? Да 

### Находка №7
- отчет flat показал, что 62% времени занимают вызовы Array#map
- Сделал подготовленный массив сессий пользователя
- метрика уменьшилась с 1.5 до 1.3 секунды
- отчет graph показал уменьшение числа вызовов с 33512 до 9142
  исправленная проблема перестала быть главной точкой роста? нет
    
### Находка №8
- отчет callstach показал, что 33% самой емкой операции занимают Date.parse
- заменил его на Date.strptime
- на метрике не видно
- отчет callstack показал что конвертация дат стала занимать вдвое меньше времени
  исправленная проблема перестала быть главной точкой роста? нет
  
- Изменил метрику на "время обработки 300 000 строк", сейчас это 6.5 секунд

### Находка №9
- отчет callstack показал, что преобразоватние дат все равно самая трудоемкая операция
- убрал преобразоваение дат
- метрика уменньшилась на 0.5 секунд
- отчет callstack показал что сбор отчетов уменьшился на 10%
  исправленная проблема перестала быть главной точкой роста? нет

### Находка №10
- происмотрелся к коду - преобразование дат не нужно
- убрал преобразоваение дат
- метрика уменньшилась на 0.5 секунд
- отчет callstack показал что сбор отчетов уменьшился на 10%
  исправленная проблема перестала быть главной точкой роста? не знаю
  
### Находка №11
- Вызов reverse при сборе отчетов лишний 
- Заменил sort.reverse на вызов sort с условием сортировки
- на метрике незаметно
- отчет callstack показал что сбор отчетов увеличился на 3% вместо reverse вызывается сравнение строк, что занимает больше времени
- вернул как было

### Находка №12
- отчет callstack показал, что сбор отчета по пользователям все равно самая трудоемкая операция
- неоптимизирована запись хеша отчета по пользователю, оптимизровал
- метрика уменьшилась на 0.5 секунд, до 5.5
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? Да.
  Как оптимизировать отчеты пока не вижу, буду оптимизировать следующие места.

### Находка №13
- При созданиие объектов user_objects массив user используется 1 раз и проходится методом each 
- заменить на map
- метрика уменьшилась до 2.7
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? да

### Находка №14
- При созданиии отчета используется each 
- заменить на map
- метрика уменьшилась до 2.3
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? как оптимизировать отчеты и загрузку объектов пока не вижу, буду оптимизировать следующие места.

### Находка №15
- Сбор уникальных бразеров занимает 9% времени 
- переписать фунцкию 
- метрика не изменилась
- как изменился отчёт профилировщика - теперь Сбор уникальных бразеров занимает 4% времени 

- Изменил метрику на "время обработки 500 000 строк", сейчас это 4.8 секунд

### Находка №16
- Увеличил объем данных профилировщиков Обнаружилось что 20% времени используеет JSON. 
- заменить на оптимизированную библиотеку oj
- метрика уменьшилась до 3.5
- как изменился отчёт профилировщика - теперь операция занимает 3% времени. 
  исправленная проблема перестала быть главной точкой роста? Да

### Находка №17
- Все еще самое большое время занимает Array#map 
- нашел лишние еще лишние вызовы в отчетах, убрал
- метрика уменьшилась до 3.3
- как изменился отчёт профилировщика - в отчетах занимали 16% времени теперь 5%

### Находка №18
- Вызовы parse_session и parse_session занимют 20% времени, но нужны лишь сводки по ним
- Вставил анализ сессий в создание пользователя и собрал там нажные параметры
- метрика уменьшилась до 2.1
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста? да

## Результаты
В результате проделанной оптимизации наконец обработать файл с данными.
В данный момент метрика "время обработки файла с 3 250 940 строками" составляет 22 секунды,
Что укладывается в заданный бюджет.

Надо отдельно отметить важность учета работы сборщика мусора при работе с большими объемами данных.
С одной стороны выключение его может вызвать аварийное завершение программы, с другой стороны,
его работа может серьезно замедлять работу программы. Так выключение сборщика мусора снижает вряме обработки до 11 секунд. 
  
На небольших данных его работа занимает гораздо меньший процент времени.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан performance-тест.
Для увеличения скорости получения результатов была выбрана метрика "время обработки файла с 500 тысячами срок" 
и задана граница времени в 2.2 секунд. 
Таким образом тест производительности c прогревом и двумя повторами занимает около 7 секунд, 
что я считаю хорошим показателем для получения обратной связи.
