# Case-study оптимизации

## Актуальная проблема

В файле `task-1.rb` находится ruby-программа, которая выполняет обработку данных из файла.

С помощью этой программы нужно обработать файл данных `data_large.txt`.

Проблема заключается в том, что это происходит слишком долго

Задачей является оптимизация программы до такой степени, чтобы программа корректно отрабатывала файл `data_large.txt` за `30 сек`

## Формирование метрики

Для понимания того, что мои изменения дают положительный эффект на быстродействие программы, я решил использовать такую метрику:
- использовать профилировщики `ruby-prof`, `stackprof`, `rbpsy` для анализа производительности на разных объемах данных
- использовать ProgressBar для понимания скорости работы программы и процента выполненной работы

## Гарантия корректности работы оптимизированной программы

Гарантией работы программы является ее тестирование. Каждый раз перед завершением и во время работы над очередной итерации будет выполняться тестирование логики работы программы на тестовых данных, подготовленных в файле `data.txt`

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 3-4 минуты

Вот как я построил `feedback_loop`:
- сделал отчет с помощью одного из профилировщиков
- зафиксировал время исполнения
- нашел точку роста (место, которое занимает много времени на исполнение)
- попробовал найти решение с помощью https://github.com/fastruby/fast-ruby и других ресурсов
- внес изменения в код
- сделал отчет с помощью одного из профилировщиков
- зафиксировал время исполнения
- проанализировал повлияли ли изменения на быстродействие программы
- запустил тесты
- если изменения повлияли положительно и при этом прошли все тесты, то приступаю к следующей итерации

## Вникаем в детали системы, чтобы найти главные точки роста

Для того, чтобы найти "точки роста" для оптимизации я воспользовался:

- Подготовил файлы для профилировщиков с разным объемом данных с помощью команды:
```
head -n N data_large.txt > data_N.txt
```

- Проверил быстродействие с помощью `rbspy`
Понял, что мне неудобно постоянно запускать процесс, искать его pid и подключаться еще к нему.
При этом сам отчет показался слишком расплывчатым без точных деталей.

- Проверил быстродействие с помощью `ruby-prof`

- Проверил быстродействие с отключенным GC

- Проверил быстродействие с помощью `stackprof`

Вот какие проблемы удалось найти и решить:

### Ваша находка №1
- С помощью `stackprof` посмотрел быстродействие системы.
```
21  (   72.4%)  Array#each
```
- Посмотрел все места с each и понял, что в некоторых местах each используется не рационально:

https://github.com/fastruby/fast-ruby#enumerableeach--push-vs-enumerablemap-code

```
Comparison:
            Array#map:   158090.9 i/s
   Array#each + push:    99634.2 i/s - 1.59x slower
```

- Исправил в некоторых местах `each + push` на `map`

- метрика изменилась:
```
16  (   66.7%)  Array#map
9  (   37.5%)  Array#select
3  (   12.5%)  Object#collect_stats_from_users
3  (   12.5%)  Array#each
```

Время исполнения снизилось с ~`3.673483s` до ~`2.516519s` для файла в 10000 строк

### Ваша находка №2
- С помощью `stackprof` speedscore посмотрел быстродействие системы.
```
1.54s (85%)	360.04ms (20%)	Array#select
```
Array#select занимал большую часть времени
Потратил несколько часов, копаясь в различных форумах, статьях с возможностью оптимизировать select
Понял, что копал не туда и нашел решение в структуре исходных данных
Оказывается, что select там просто не нужен
Структура данных однотипна, за информацией о пользователе всегда идет список сессий, привязанных к нему, поэтому select здесь излишен

- Вместо использования select перешел на проверку наличия слова `user` для определения строки, содержащей информацию о пользователе, последующие строки записывал в сессии этого пользователя
- Провел рефакторинг, вместо того, чтобы каждый раз делать `users.each` на каждое поле для вывода результата, сделал один `each` для сбора значний всех полей.
- Ускорил конкатенацию строк в присваивании user_key, исходя из документации: https://github.com/fastruby/fast-ruby#string-concatenation-code

- После внесения изменений время исполнения для файла в 10000 строк снизилось до ~`1.277132s`
Array#select больше не присутствовал в отчете профилировщика

### Ваша находка №3
- С помощью `stackprof` speedscore посмотрел быстродействие системы
```
15.31ms (32%)	15.31ms (32%)	String#split
```
String#split занимал ~32% всего времени
- Убрал лишние вызовы split в методах `parse_session` и `parse_user`

- Из отчета профилировщика split стал занимать меньше времени
```
8.38ms (16%)	8.38ms (16%)	String#split
```
- После внесения изменений время исполнения для файла в 10000 строк снизилось до ~`0.946666s`

### Ваша находка №4
- Внедрил ProgressBar
- Он влияет на производительность, но позволяет сделать примерную оценку и показывает статус текущего процесса

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
