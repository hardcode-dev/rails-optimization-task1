# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
все отчеты показывали примерно один результат, по этому основным ориентиром взял отчет graph_report

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 14 секунд

Вот как я построил `feedback_loop`: я создал файл на 90000 строк, выполнение кода с этим количеством строк занимало 14 секунд, что позволит увидеть изменения во времени выполнения.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- использовал отчет graph_report, точка роста в обработке массива
- ускорить обработку массива, разместить всю информацию по сессиям, сразу в пользователе. чтобы не было повторной выборки из массива
- на файле в 90000 строк, выполнение программы стало 2 секунды
- главная точка роста сместилась

### Ваша находка №2
- пришлось увеличить тестовый файл до 300000 строк, так как на меньшем количестве строк, код выполняется довольно быстро. использовал отчет graph_report, точка роста в обработке файла
- попробовал читать файл не весь, а по строкам
- на этом количестве строк, прирост был не значителен, 2 секунды. решил проверить на файле data_large, считка и обработка занимает теперь 26 секунд. но оказывается, много времени занимает вывод информации.
- метрика улучшилась в лучшую сторону
- главная точка роста сместилась


### Ваша находка №3
- пришлось увеличить тестовый файл до самого большого файла, так как на меньшем количестве строк, код выполняется довольно быстро. использовал отчет graph_report, точка роста в выводе информации в методе collect_stats_from_users
- я решил собирать информацию по мере обработки строк файла, чтобы в конце не надо было все считать еще раз
- ощутимых изменений нет
- главна точка роста сместилась снова на перебор строк файла. на пару процентов отстает перебор юзеров в collect_stats_from_users. т.к. не знаю как можно улучшить работу с файлом, попробую оптимизировать перебор пользователей

### Ваша находка №4
- пришлось увеличить тестовый файл до самого большого файла, так как на меньшем количестве строк, код выполняется довольно быстро. использовал отчет graph_report, точка роста в выводе информации в методе collect_stats_from_users
- переделал метод сбора информации по пользователям, время выполнения этого метода уменьшилось
- ощутимых изменений нет
- главна точка роста сместилась снова на перебор строк файла, 36 секунд. следующий за ним метод парсинг сессии - 20 секунд
- если исключить замеры и убрать прогресс бар, то общее время получается 30,5 секунд

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с неизмерного времени обработки data_large до 30,5 секунд и почти уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан performance_test, который не позволяет ухудшить метркику выполнения кода

