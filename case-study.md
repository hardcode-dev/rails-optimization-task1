# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.
Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время преобразования текстового файла в json по предложенному тестовому образцу.
Перед внесением изменений в программу было измерено произведен анализ асимптотики для тестовых файлов размером 2500, 5000, 10000 и 20000 строк

Файл с 2500 строками обработался за 0.22 sec
Файл с 5000 строками обработался за 0.79 sec
Файл с 10000 строками обработался за 3.07 sec
Файл с 20000 строками обработался за 12.05 sec

Отключение сборщика мусора дало падение производительности на увеличивающихся объемах данных:
Файл с 2500 строками обработался за 0.21 sec
Файл с 5000 строками обработался за 1.01 sec
Файл с 10000 строками обработался за 4.27 sec
Файл с 20000 строками обработался за 16.36 sec

Беглый анализ асимптотики показал экспоненциальный рост - примерно O(N^2)
Таким образом, исходный файл с 3_250_940 строками будет обрабатываться более 283_920 секунд (более 3 дней)

Бюджет метрики - преобразовывать текстовый файл с ~3_000_000 строк в json за 30 и менее секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за две-три минуты
Для проверки гипотез я начал с файла в 10000 строк как приемлемый по начальному времени обработки, но в то же время содержащий достаточное количество данных
Так же я написал тест, защищающий время обработки от регрессии

Вот как я построил `feedback_loop`:
- произвести замер времени обработки файла
- произвести профилирование и выявить точку роста
- попытаться утранить найденную точку роста оптимизировав код
- выполнить тестирование корректности и скорости обработки файла
- произвести замер времени обработки файла, чтобы убедиться, что оптимизация помогла
- зафиксировать новое максимальное время выполнения в тесте

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- ruby-prof в режимах CallStack, CallGrind и Flat a так же stacprof в режиме CLI

Вот какие проблемы удалось найти и решить:

### Находка №1
- Файл с 10000 записей обрабатывался 3.1 секунды
- Отчет показал, что точкой роста является метод `Array#select` который занимал 84.50% времени обработки файла
- Оптимизация: метод `select` выполняется над массивом `sessions` столько раз, сколько элементов содержит массив `users`. Необходимо заменить вызов метода `select` группировкой массива `sessions` по `user_id` до перебора массива `users` и передавать внутрь перебора уже сгруппированные данные
- Тесты корректности и скорости обработки файла были пройдены
- В результате время выполнения программы снизилось до 0.36 секунды

### Находка №2
- Файл с 50000 записей обрабатывался 10.43 секунды
- Отчет CallStack показал, метод `Array#each` занимает 89.86%. Stackprof CLI уточнил что это `file_lines.each`. В методе происходит преобразование каждого элемента массива (строки файла) в отдельный массив, а также передача оригинальных элементов массива (строк) в метод `parse_session` либо `parse_user`, где происходит повторное преобразование элемента в массив.
- Оптимизация: передавать в дочерние методы уже преобразованные элементы, а так же заменить сложение элементов массивов `users` и `sessions` на метод добавления элементов в массив `push`
- Тесты корректности и скорости обработки файла были пройдены
- Избавление от дублирования метода `split` снизило время выполнения программы до 8.82 секунды. Замена метода добавления элементов в массив на `push` снизило время выполнения программы до 1.07 секунды. Дополнительная замена двойного `if` на `case` дало 1.02 секунды выполнения программы.

### Находка №3
- Файл с 250000 записей обрабатывался 10.01 секунды
- Отчет CallStack показал, метод `Array#each` занимает 89.86%. Stackprof CLI уточнил что это `users.each`. Внутри метода был обнаружено создание экземпляра класса `User` и добавление объектов в массив `users_objects` суммированием.
- Оптимизация: метод добавления элементов в массив `push`, избавиться от излишнего класса `User`, заменив `user_object` на хэш.
- Тесты корректности и скорости обработки файла были пройдены
- Замена метода добавления элементов в массив на `push` снизило время выполнения программы до 4.73 секунды. Избавление от класса `User` не дало сильного результата, но все же снизило время обработки до 4.67 секунд.

### Находка №4
Во время замены класса `User` на хеш и его ключи, были найдены строки, которые использовали суммирование как способ получить конкатенации. Было решено попробовать обойтись без суммирования, заменив, к примеру,
`"#{user[:attributes]['first_name']}" + ' ' + "#{user[:attributes]['last_name']}"` на `"#{user[:attributes]['first_name']} #{user[:attributes]['last_name']}"`
Оптимизация дала еще небольшой прирост скорости, сократив время обработки 250000 строк до 4.54 секунд

### Находка №5
В том же методе обнаружился неоптимальный способ смержить два хеша. По аналогии с суммированием элементов массива, метод `merge` был заменен на `merge!` с удалением пересохранения исходного хэша, это дало еще 10мс, сократив время обработки 250000 строк до 4.45 секунд

### Находка №6
- Файл с 500000 записей обрабатывался 9.09 секунды
- Отчет CallStack показал, метод `Array#each` занимает 52.85%. Его дочерний метод `Array#all?` занимает 42.23% времени обработки файла. Метод `all?` выполняется над массивом `uniqueBrowsers` столько раз, сколько элементов содержит массив `sessions`.
- Оптимизация: Для получения уникальных браузеров достаточно сгруппировать массив `sessions` по `browser` и преобразовать ключи полученного хеша в массив.
- Тесты корректности и скорости обработки файла были пройдены
- В результате время выполнения программы снизилось до 6.3 секунды

### Находка №7
- Отчет CallStack показал, что точкой роста является метод `Object#collect_stats_from_users ` который занимает 74.92% времени обработки файла. Stackprof CLI уточнил, что его дочерний метод `<Class::Date>#parse` является самым прожорливым.
- Оптимизация: Проанализировав строку `'dates' => user.sessions.map{|s| s['date']}.map {|d| Date.parse(d)}.sort.reverse.map { |d| d.iso8601 }` я понял что происходит преобразование строкового представления даты в собственно формат даты, затем полученный массив сортируется в обратном направлении и даты снова преобразуются в строки в формате iso8601. Сделав допущение, что исходный файл для обработки выгружается какой-то системой, что значит, что формат даты у всех записей будет единым, я посмотрел на даты в исходном файле и понял, что они уже соответствуют формату iso8601. Соответственно, решением было убрать излишние преобразования даты и ее парсинг
- Тесты корректности и скорости обработки файла были пройдены
- В результате время выполнения программы снизилось до 3.75 секунды

### Находка №8
- Файл с 1_000_000 записей обрабатывался 7.98 секунды
- Отчет показал, что точкой роста снова является метод `Object#collect_stats_from_users ` который занимает 55.84% времени обработки файла. В методе происходит множество выховов вызов метода `collect_stats_from_users` и `Array#map`.
- Оптимизация: Было решено объединить все вызовы `collect_stats_from_users` в один и попытаться уменьшить количество вызываемых методов `map`. Для этого вызовы методов `upcase` и `to_i` были подняты до уровня подготовки данных в методе `parse_session`.
- Тесты корректности и скорости обработки файла были пройдены
- Объединение всех вызовов `collect_stats_from_users` дало 6.46 секунд. Интерполяция строк `totalTime` и `longestSession` показала 6.38 секунд. Вынос `upcase` и `to_i` в `parse_session` и избавление от лишнего `map` и `to_s` улучшило результат до 6.26 секунд

### Находка №9
Было обнаружено, что получение списка всех браузеров чем-то напоминает получение числа уникальных браузеров. Было решено попробовать получить число уникальныз браузеров из списка всех браузеров.
Оптимизация привела к 5.65 секундам на файл в миллион строк и 12.13 для файла в 2_000_000 строк

### Находка №10
Т.к. метод `upcase` был вынесен в `parse_session`, это позволило удалить все оставшиеся `upcase` и `map` в вызове метода `collect_stats_from_users`. Так же проверка регулярных выражений была заменена на метод `match?`. Замена `any?` на `map` с проверкой массива на отсутствие элементов дало прирост скорости, замена же `all?` на `map` дало регрессию и это изменение было отвергнуто.

Итоговая проверка скорости обработки исходного файла показала 17.76 секунды

### Результаты
После всех изменений программы удалось добиться обработки файла за менее, чем 20 секунд вместо более чем трех суток.
