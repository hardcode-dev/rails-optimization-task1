# Case-study оптимизации

## Актуальная проблема

В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными на приблизительно 130 мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики

Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: изначальная обработка файла на 50000 строк с последующим увеличением с шагом x2.

Изначально было:

- 25_000 строк - 5.08s
- 50_000 строк - 25.01s
- 100_000 строк - 116.30s

Ориентировочно это выходит O(n^2.2).

## Гарантия корректности работы оптимизированной программы

Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop

Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за (вначале) 25s.

Вот как я построил `feedback_loop`: прогон тестов -> прогон профилировщиков -> анализом времени выполнения и главной точки роста -> внесение изменений в код -> прогон тестов и так по кругу

## Вникаем в детали системы, чтобы найти главные точки роста

Для того, чтобы найти "точки роста" для оптимизации в основном я пользовался ruby-prof. Для исследования всех типов отчетов профилировщик был настроен в режиме MultiPrinter.

Вот какие проблемы удалось найти и решить

### Медленный select в "Статистика по пользователям" (сэмпл 50_000 строк)

- ruby-prof + rbspy
- создать дополнительный хеш с ключем в видe user_id для достижения O(1)
- было: 25.01s, стало: 0.93s
- исправленная проблема перестала быть главной точкой роста

После этого я увеличил размер файла для тестовых прогонов до 200_000 строк.
Он обрабатывался за 10.81s.

### Медленный главный each с чтением файла (сэмпл 200_000 строк)

- ruby-prof
- изменил с `+= []` на `<<`
- было: 10.81s, стало: 2.70s
- исправленная проблема осталась главной точкой роста, но идеи на этот момент закончились

После этого я увеличил размер файла для тестовых прогонов до 400_000 строк.
Он обрабатывался за 6.53s.

### Проверка всего массива на уникальность через метод all? в 89 строке (сэмпл 400_000 строк)

- ruby-prof
- переделал на Set
- было: 6.53s, стало: 5.37s
- исправленная проблема перестала быть главной точкой роста

После этого я увеличил размер файла для тестовых прогонов до 800_000 строк.
Он обрабатывался за 23.14s.

### Неоптимальный метод map в 147 строке (сэмпл 800_000 строк)

- ruby-prof
- неоптимальное использование map и лишний парсинг даты, ведь получить нужно было только строку даты
- было: 23.14s, стало: 21.20s
- исправленная проблема перестала быть главной точкой роста

### Неоптимальный метод map в 97 строке (сэмпл 800_000 строк)

- ruby-prof
- убрать 1 map, создать Set
- было: 21.20s, стало: 21.10s
- исправленная проблема перестала быть главной точкой роста

### Неоптимальные map в 124 и 134 строках (сэмпл 800_000 строк)

- ruby-prof
- объединить по 2 map в по 1му
- было: 21.10s, стало: 21.10s
- исправленная проблема перестала быть главной точкой роста, но время если и изменилось, то незначительно

### Решил глобально подойти к главному each (сэмпл 800_000 строк)

- ruby-prof
- пересобрать главный each
- было: 21.10s, стало: 2.55s
- исправленная проблема перестала быть главной точкой роста и уложился в бюджет

## Результаты

В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 25.01s с файлом на 50_000 строк до 15.65s с финальным файлом на 3_250_940 строк и уложиться в заданный бюджет.

## Защита от регрессии производительности

Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан тест производительности с тестовым файлом на 25_000 строк и ограничением по времени исполнения в 70ms.
