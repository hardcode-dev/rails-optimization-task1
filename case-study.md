# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему и оптимизировать эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы будем использовать простую метрику - **время обработки файла**

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Выстраиваем работу и Feedback-Loop

* Метод для обработки файлов `work` и тест для него находятся в одном файле - что не удобно. Выделил тест в отдельный файл. Для удобства запуска вызов метода также сделал в отдельном файле
* Добавил `Gemfile` через `Bubndle` с необходимыми `gem`'ами, зафиксировал версию `ruby`
* Добавил параметр с именем файла `filename` в метод  `work`
* Ненужные папки и файлы внес в  `.gitignore`
* Исходный файл имеет более 3х миллионов строк `$ wc -l tmp/data_large.txt # 3250940 tmp/data_large.txt` и обрабатывается неприлично долго, поэтому нарезал файлы поменьше - такие, на которых программа отрабатывает достаточно быстро - это позволит нам выстроить фидбек-луп. Решение:
```ruby
range = [1000, 2000, 4000, 8000, 10_000, 16_000, 20_000]
range.each do |n|
  `head -n #{n} tmp/data_large.txt > tmp/data_#{n}.txt`
end
# =>
# tmp/data_1000.txt
# tmp/data_2000.txt
# tmp/data_4000.txt
# ...
```
* Сделал небольшой скрипт с `Benchmark.realtime` для подсчета времени обработки полученных выше файлов. Получилось следующее:
```
$ ruby benchmark/benchmark_realtime.rb
1000 Finish in 0.03
2000 Finish in 0.08
4000 Finish in 0.27
8000 Finish in 0.94
10000 Finish in 1.43
16000 Finish in 3.37
20000 Finish in 5.14
в секундах
```
Видно, что асимптотика роста времени работы в зависимости от объёма входных данных не линейна - скорее всего полиномиальная и больше O(N^2)

Интересна оценка сколько программа будет работать на полном объеме данных. Если все же предположить что у имеющегося алгоритма полиномиальный рост в N^2, то получится, что 3.2 миллиона строк будут обрабатываться не меньше чем 10_240_000 секунд, что примерно 120 дней, что не укладывается в метрику

### Защита от регрессии производительности
Для защиты от регрессии производительности воспользуемся гемом `rspec-benchmark`
В качестве опорной точки возьмем файл в 10_000 строк и напишем тест на метрику время обработки не более чем 2 сек
```ruby
expect {
  work('tmp/data_10000.txt')
}.to perform_under(2000).ms.warmup(2).times.sample(3).times
```

###  Профилирование
При профилировании лучше выключать  `GC`  (он может вносить непредсказуемые замедления в рандомные части программы). Поэтому, в метод `work` добавим параметр `disable_gc`.

Для профилирования я выбрал gem `ruby-prof`, который строит удобные html и txt отчеты. При запуске обработки через `ruby-prof` отключаем `GC` и строим отчеты в форматах `FlatPrinter`, `GraphHtmlPrinter` и `CallStackPrinter`

### Feedback-Loop
В итоге у меня получился эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений менее чем за 10 сек.

Скрипты положил `Makefile`:
```
.PHONY: test
test:
  ruby test/task_1_test.rb
  rspec test/performance_spec.rb
bench:
  ruby benchmark/benchmark_realtime.rb
  ruby benchmark/ruby-prof.rb
```

## Вникаем в детали системы, чтобы найти главные точки роста

### Находка №1
После анализы отчетов `ruby-prof` стало понятно, что главная точка раста программы - это вызовы `Array#select`, который занимал 85% времени обработки.
Этот метод в вызывался в одном месте, но, чтобы убедиться, я вынес этот вызов в отдельный метод и повторил профилирование.

Проблема была в том, что для каждого пользователя выбирались его сессии из всего списка
Применив мемоизацию (сохраняем сессии пользователя в хеше user) время обработки существенно сократилось и асимптотика приблизилась к линейной

```
$ ruby benchmark/benchmark_realtime.rb
1000 Finish in 0.02
2000 Finish in 0.03
4000 Finish in 0.07
8000 Finish in 0.15
10000 Finish in 0.2
16000 Finish in 0.31
20000 Finish in 0.42
```

Тесты проходят, ситуация в профилировщике `ruby-prof` изменилась - теперь можно обновить тест для защиты от регрессии производительности.

### Находка №2
В новых отчетах профилировщика `ruby-prof` видно, что теперь главная точка роста - `Array#all?` - Подсчёт количества уникальных браузеров
```
Total: 0.585065
Array#all? 35%
10000 Finish in 0.16
20000 Finish in 0.38
40000 Finish in 0.88
80000 Finish in 1.89
160000 Finish in 3.64
```
Оптимизируем подсчет применив `Set`, попутно поменяем allBrowsers на использование этого множества
```
Total: 0.395011
Array#all? - ушел из топа нагрузки
10000 Finish in 0.12
20000 Finish in 0.29
40000 Finish in 0.7
80000 Finish in 1.52
160000 Finish in 2.99
```

Прирост производительности достигнут. Тесты проходят, ситуация в профилировщике `ruby-prof` изменилась - теперь можно обновить тест для защиты от регрессии производительности и сделать commit.

### Находка №3

По аналогии с предыдущими находками продолжаем: Смотрим отчеты, понимаем что главная точка роста - многократные вызовы `Array#each` и `collect_stats_from_users`. Рефакторим.

### Находка №4

Главная точка роста - многократные вызовы `Array#map` в `collect_stats_from_users`

Также  много процессорного времени потребляет `Date#parse`. В исходном файле даты в формате iso8601. Этот формат хорош тем, что если применить сортировку к строкам, результат будет такой же, как и при сортировки дат. Поэтому парсинг дат можно убрать - он значительно замедляет работу приложения.

при рефакторинге исключил лишние `Array#map` - получил прирост почти в 2а раза.

### Находка №5

Главная точка роста - многократные вызовы `String#split`

### Находка №6

После несколькиз итераций рефакторинга файл `data_large.txt` обрабатывается примерно 30 сек - что укладывается в **бюджет** выбранной метрики.
Одной из главных точек роста является формирование JSON. Выбрав для парсинга `gem` `oj` - наблюдал почти 20% прироста

## Результаты

В результате проделанной оптимизации удалось обработать файл с данными. 
Удалось улучшить метрику системы с "бесконечности" до 25 секунд и уложиться в заданный бюджет. Если отключить GC программа выполняется за 12 секунд, но лучше сборщик мусора не выключать, чтобы избежать утечек памяти.

Заметил, что существующий тест на корректность не проверяет `alwaysUsedChrome`
Также обратил внимание на то, что браузеры и даты сессий по каждому пользователю и в тесте и в исходной программе не уникальны - возможно заказчик не обратил внимание на уникальность таких данных в отчете.

Метод  `collect_stats_from_users` сейчас занимает 40% времени. Этот метод можно оптимизировать, и посчитать статистические данные при первом проходе по строкам файла, но прирост производительности не окупит вложенных усилий - прямое нарушение правила Парето.

**Отдельно обращую внимание на:**

- Если файл `text_large.txt` является выгрузкой из базы, то было бы эффективнее обработать данный запрос на уровне БД.