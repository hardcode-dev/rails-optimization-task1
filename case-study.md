# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
Я разбил большой файл на три файла: data_1000.txt, data_10000.txt, data_30000, где цифры - это количество строк в файле.
После чего я подключил гем brenchmark чтобы оценивать потраченное время на обработку каждого файла. А так же гем rspec-benchmark для тестирования. С каждым результатом я ставил ожидаемое время меньше и меньше, дабы избежать регрессии.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *30 секунд (?)*

Вот как я построил `feedback_loop`:
  - Прежде всего я разбил data_large.txt на другие файлы с меньшим содержанием строк - это файлы с 1000, 10000 и 30000 строками. И поместил их в директорию fixtures.
  - В директорию raport я поместил результаты профилировщиков.
  - Внутри метода work я установил benchmark для показа затраченного времени
  - Два вида тестов на производительность: это тесты с включенным сборщиком мусора и выключенным, чтобы увидеть сколько времени забирает сборщик мусора. На первых порах тестирование проводилось на тестах с отключенным сборщиком мусора. Сами тесты включают в себя метрику, где устанавливаются предел времени за который нельзя выходить при выполнении работы.
  - А также файлы профилировщиком, которые генерируют отчет

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
  - гем ruby-prof
  - rbspy
  - kcachegrind
  - rubocop-performance
  - ruby 3.0.2

Вот какие проблемы удалось найти и решить

## Изначальное положение дел
Для начала я запустил профилировщик rbspy который указал на то, что вся работа (98%) приходится на метод work. Но поскольку work слишком большой метод, который в себе содержит разнообразную логику, решено было разбить этот метод на другие методы, тем самым делегировать задачи.
Я не догадался по началу сохранять все метрики, а записывал в общих числах, при этом сборщик мусора был отключен как указывалось выше лишь на начальных этапах.
- data_1000.txt - 0.4ms
- data_10000.txt - 2s
- data_30000.txt - 28s

### Ваша находка №1
- Для начала я решил запустить профилировщик ruby_prof. Для отображения я использовал графический режил в виде HTML файла. И для прогона я использовал файл в 10_000 строк. Он указал, что чтение файла занимает большую нагрузку.
- Изменил метод чтения с `File.read` на `File.readlines(filanme).map do`, по той причине, что мне показалось когда каждая строка начинает сразу парсится, будет явно процесс идти быстрее нежели сначала метод загрузит все данные, а потом начнет парсить.
- Метрика не сильно изменилась, буквально на несколько миллисекунд

### Ваша находка №2
- Запустил вновь профилировщик и он указал на точку роста в методе Array#select. Получается, что вот эта операция `user_sessions = sessions.select { |session| session['user_id'] == user['id'] }` не самая лучшая. Попробую что-то с ней сделать.

Изменил `select` на `reject`, показатели улучшились. Сама по себе эта часть долгой для меня была, не мог понять каким образом оптимизировать. Решил для начала вывести данные, которые хранят в себе переменные sessions и users, и обнаружил, что в sessions данные дублируются, и проверка на Id прогоняются по много раз, что естественно является избыточным. Поэтому решено было сгруппировать значения по значениям ключа, за счет метода `group_by`.

Также изменил способ добавления в массив, операция `<<` считается быстрым чем обычное присвоение, вроде `+=`. По той причине, что во втором случае он возвращает сперва копию и потом присваивание.

Показали запуска ошеломили!
Если до этого в тестах предел для data_1000 было 0.4 миллисекунды, дата 10_000 не более двух секунды, 30_000 не более 28 секунд, то теперь показатели улучшились.

- 1_000 - 0.02ms
- 10_000 - 0.16ms
- 30_000 - 0.6ms

Но эти показатели тестировались на отключенном сборщике мусора, на включенном результат удручает. Поэтому буду продолжать искать точки роста.  Так или иначе сборщик мусора влияет на скорость выполнения задачи, нужно как-то искать решение.

С включенным сборщиком мусора результат выглядит в данный момент вот так:

- data - 0ms
- data_1000 - 0.01ms
- data_10000 - 0.75ms
- data_30000 - 28ms

Поставил новые значения тестам.

### Ваша находка №3
- Далее профилировщик указывает на нагруженность map в методе `collect_stats_from_users`. Также я вижу по коду, что метод `collects_stats_from_users` вызывается множество раз. Пробую оптимизировать это.

Я не вызываю каждый раз метод `collecti_stats_from_users`, а вызываю один раз и засовываю вычитание данных пользователя в один блок.

Также дублировался код в вычитании времени для ключей `totalTime` и `longestSession`. Решил все это вынести одной операцией дабы избежать дублирующего итератора

Тоже самое обнаружил и для вычитания статистики для браузеров. Также в трех местах избыточный map, поэтому сделал одну коллекцию браузеров.

Должного эффекта эти манипуляции не дали. На данный момент, вот статистика моя:

Со сборщиком мусора:
- 1000 - 0.01ms
- 10_000 - 0.75ms
- 30_000 - 27ms

Без сборщика мусора:
- 1000 - 0.01ms
- 10_000 - 0.1ms
- 30_000  - 0.38ms

Очень наглядно поднялось время выполнение если сборщик мусора отключен. Продолжаю поиски точек роста.

### Ваша находка №3
- на этот раз запущу профилировщик Callgrind. 62 процента self занимает метод `all?`. Этот метод используется при подсчете количества уникальных браузеров. Я изменил его на метод include? поскольку браузер добавляется в массив при условии, что его еще нету в самом массиве.

Прогнав тесты бенчмарков - это дало свой результат
Со сборщиком мусора:
- 1000 - 0.01ms
- 10_000 - 0.72ms
- 30_000 - 27s

Без сборщика мусора:
- 1000 - 0.01ms
- 10_000 - 0.07ms
- 30_000  - 0.29ms

К сожалению не удается пока что оптимизировать выполнение работы со сборщиком мусора. Продолжаю дальше исследовать.

### Ваша находка №4
- Профилировщик указал, что метод перебоя мап забирает 33 процента работы. Он вызывается для парсинга даты.

Date.parse поменял на другой метод - Date.strptime, произошли небольшие изменения

Со сборщиком мусора:
- 1000 - 0.01ms
- 10_000 - 0.69ms
- 30_000 - 25s (Вообще должен сказать, что время от времени показатель прыгает, от 24 до 27)

Без сборщика мусора:
- 1000 - 0.01ms
- 10_000 - 0.06ms
- 30_000  - 0.27ms

Чуть выше собирается статистика по использованию браузеров, я там также избавился от регекса и добавил Include?. Большего результата это не дало.

### Ваша находка №5
- Далее я немного нарушил принцип, что нельзя в слепую действовать, но тем не менее это дало некоторый результат. Уж слишком некоторые вещи мне бросались в глаза, которые я решил все таки оптимизировать.

Я обнаружил, что часть итераций повторяется и некоторые действия можно засунуть в один итератор, так произошло с итераторами которые собирают в массив уникальные браузеры и преобразуют название этих браузеров в заглавные буквы

Также метод `collect_stats_from_user` я убрал в `record_user_statistics`, а именно убрал в итератор пользователей.

*Чтобы не громоздить я буду впредь выводить результаты файла на 30_000 строк и data_large.txt*

Запускаю тесты со сборщиком мусора:
```
.Time for fixtures/data_30000.txt is 27.74
Time for fixtures/data_30000.txt is 24.31
Time for fixtures/data_30000.txt is 25.4
Time for fixtures/data_30000.txt is 25.43
Time for fixtures/data_30000.txt is 26.87
Time for fixtures/data_30000.txt is 28.12
Time for fixtures/data_30000.txt is 24.02
Time for fixtures/data_30000.txt is 25.81
Time for fixtures/data_30000.txt is 25.83
Time for fixtures/data_30000.txt is 25.85
Time for fixtures/data_30000.txt is 24.44
Time for fixtures/data_30000.txt is 24.22
```

Без:
```
Time for fixtures/data_30000.txt is 0.26
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.25
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.25
Time for fixtures/data_30000.txt is 0.24
Time for fixtures/data_30000.txt is 0.24
```

### Ваша находка №6
- Результат уже лучше. Запускаю профилировщик опять.
Из результатов я так понимаю, большее время до сих пор занимает парсинг времени. Хоть в прошлый раз показатели улучшились тем, что я вместо `parse` использовал `strptime`, все равно нужно как-то оптимизировать эту итерацию map. С этим я еще не решил, что пока делать, но я обнаружил одну вещь, а именно во время чтения данные в массив добавляются таким образом:
```
    File.readlines(filename).map do |line|
      cols = line.split(',')
      users = users + [parse_user(line)] if cols[0] == 'user'
      sessions = sessions + [parse_session(line)] if cols[0] == 'session'
    end
```

Я изменил эту логику на следующую:
```
    File.readlines(filename).map do |line|
      cols = line.split(',')
      users << parse_user(line) if cols[0] == 'user'
      sessions << parse_session(line) if cols[0] == 'session'
    end
```

Запускаю тесты и такой у меня результат:
Без сборщика:
```
  Time for fixtures/data_30000.txt is 0.15
  Time for fixtures/data_30000.txt is 0.17
  Time for fixtures/data_30000.txt is 0.15
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.15
  Time for fixtures/data_30000.txt is 0.15
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.14
  Time for fixtures/data_30000.txt is 0.14
```

Со сборщиком:
```
  Time for fixtures/data_30000.txt is 0.21
  Time for fixtures/data_30000.txt is 0.22
  Time for fixtures/data_30000.txt is 0.23
  Time for fixtures/data_30000.txt is 0.21
  Time for fixtures/data_30000.txt is 0.23
  Time for fixtures/data_30000.txt is 0.23
  Time for fixtures/data_30000.txt is 0.23
  Time for fixtures/data_30000.txt is 0.23
  Time for fixtures/data_30000.txt is 0.24
  Time for fixtures/data_30000.txt is 0.24
  Time for fixtures/data_30000.txt is 0.24
  Time for fixtures/data_30000.txt is 0.26
```

**Результат улучшился почти в сто раз!!!**

Решил проверить и запарсить `data_large.txt` с включенным сборщиком мусора, и ура, результат уже такой(до этого мне не хватало терпения дождаться):
```
  Time for fixtures/data_large.txt is 40.86
  Time for fixtures/data_large.txt is 42.69
  Time for fixtures/data_large.txt is 40.06
  Time for fixtures/data_large.txt is 39.74
  Time for fixtures/data_large.txt is 48.0
  Time for fixtures/data_large.txt is 52.81
  Time for fixtures/data_large.txt is 50.97
  Time for fixtures/data_large.txt is 48.28
  Time for fixtures/data_large.txt is 51.71
  Time for fixtures/data_large.txt is 54.38
  Time for fixtures/data_large.txt is 53.32
  Time for fixtures/data_large.txt is 50.82
```

### Ваша находка №7

- Запускаю профилировщик опять
По прежнему много времени берет парсинг даты.
Я это дело перенес в один итератор, где сразу собираются необходимые коллекции данных, чтобы избежать повторных итераций мапом

```
  user.sessions.map do |s|
    time_collection << s['time'].to_i
    browsers_collection << s['browser'].upcase
    date_collection << Date.strptime(s['date'].to_s.chomp, '%Y-%m-%d').to_s
  end
```

  Также обнаружил что при парсинге делается ненужное действие:
```
      File.readlines(filename).map do |line|
      **cols = line.split(',')**

      users << parse_user(line) if cols[0] == 'user'
      sessions << parse_session(line) if cols[0] == 'session'
```

тут делается разбив через запятую методом `split` и в самих методах парсинга также делается разбив через запятую, проще вместо `line` сразу передавать `cols` дабы избежать ненужных действий в методах парсинга:

```
    File.readlines(filename).map do |line|
      cols = line.split(',')
      users << parse_user(cols) if cols[0] == 'user'
      sessions << parse_session(cols) if cols[0] == 'session'
    end
```

Запускаю data_large.txt парсинг, и результат уже немного лучше:

```
Time for fixtures/data_large.txt is 32.98
Time for fixtures/data_large.txt is 37.89
Time for fixtures/data_large.txt is 35.1
Time for fixtures/data_large.txt is 33.18
Time for fixtures/data_large.txt is 41.58
Time for fixtures/data_large.txt is 41.19
Time for fixtures/data_large.txt is 42.44
Time for fixtures/data_large.txt is 40.91
Time for fixtures/data_large.txt is 36.8
Time for fixtures/data_large.txt is 44.4
Time for fixtures/data_large.txt is 43.87
Time for fixtures/data_large.txt is 47.13
```

### Ваша находка №8
- Запускаю профилировщик. Указывает что нагрузка идет на итераторе по сборе статистики пользователя.
Убрираю мемоизацию `report['usersStats'][user_key] ||= {}`. И запускаю тесты, результаты для data_large.txt стали еще лучше:

```
Time for fixtures/data_large.txt is 33.26
Time for fixtures/data_large.txt is 36.4
Time for fixtures/data_large.txt is 34.72
Time for fixtures/data_large.txt is 33.75
Time for fixtures/data_large.txt is 39.73
Time for fixtures/data_large.txt is 41.62
Time for fixtures/data_large.txt is 35.96
Time for fixtures/data_large.txt is 32.19
Time for fixtures/data_large.txt is 40.17
Time for fixtures/data_large.txt is 38.51
Time for fixtures/data_large.txt is 34.51
Time for fixtures/data_large.txt is 43.45
```

### Ваша находка №9
- Следующая находка, что сам класс User не нужен, и не обязательно создавать объект user, чтобы передавать значения, когда уже есть для этого необходимые данные.

Я удалил класс User, а в user_key напрямую передаю данные user которые спарсины с файла, и показатели теста стали уже близки к условию задачи:
```
Time for fixtures/data_large.txt is 29.77
Time for fixtures/data_large.txt is 32.17
Time for fixtures/data_large.txt is 30.92
Time for fixtures/data_large.txt is 30.07
Time for fixtures/data_large.txt is 34.53
Time for fixtures/data_large.txt is 35.6
Time for fixtures/data_large.txt is 37.71
Time for fixtures/data_large.txt is 35.06
Time for fixtures/data_large.txt is 37.69
Time for fixtures/data_large.txt is 34.71
Time for fixtures/data_large.txt is 38.53
Time for fixtures/data_large.txt is 33.55
```

### Ваша находка №10
- В заключительной находки обратил внимание, что для хешей ключи использую обычные строки, в то время как символы могут сыграть свою роль. Я решил проверить свою гипотезу, и везде заменил ключи хеша на символы. Вот что из этого получилось:

```
Time for fixtures/data_large.txt is 30.44
Time for fixtures/data_large.txt is 32.64
Time for fixtures/data_large.txt is 32.7
Time for fixtures/data_large.txt is 29.66
Time for fixtures/data_large.txt is 29.26
Time for fixtures/data_large.txt is 30.97
Time for fixtures/data_large.txt is 28.95
Time for fixtures/data_large.txt is 30.33
Time for fixtures/data_large.txt is 30.15
Time for fixtures/data_large.txt is 29.76
Time for fixtures/data_large.txt is 28.99
Time for fixtures/data_large.txt is 28.92
```

Тем самым мне удалось отпарсить огромный файл за примерно 30 секунд, что было указано в бюджете.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с неопределенного времени (не хватило терпения дождаться) до 30 секунд и уложиться в заданный бюджет. И при этом на подобных тестах сборщик мусора был включен. И использовался руби версии три.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были написаны rspec тесты с установленной метрикой, которая была достигнута. Всего используются три теста. Первый тест в самом task-1.rb, и нужен для того, чтобы убедиться что структура отпарсинных данных соответствует ожиданию, а также два сторонних файла - в одном тесты проводятся при включенном сборщике мусора, в другом нет.

