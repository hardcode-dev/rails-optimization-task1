# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго,
и не было понятно, закончит ли она вообще работу за какое-то разумное время.
Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать
замер времени выполнения программы относительно обьема загружаемых данных.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы
при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`,
который позволил мне получать обратную связь по эффективности сделанных изменений за *12sec*

Вот как я построил `feedback_loop`. С начала я решил определить размер данным с которым я могу достаточно
быстро получить результат работы программы. Для этого я постепенно делил размер данных на два, в итоге получил список
файлов, который позволял эффективно управлять нагрузкой программы в процессе ее оптимизации. Список файлов получился
в виде: `data_large_512x.txt`, `data_large_256x.txt`, `data_large_128x.txt`, `data_large_64x.txt`, `data_large_32x.txt`,
`data_large_16x.txt`, `data_large_8x.txt`, `data_large_4x.txt`, `data_large_2x.txt`, `data_large.txt`.

Первым файлом, который откликнулся за разумное время был файл под названием `data_large_32x.txt` учитывая это я
решил отталкиваться и строить ассимтотику:

| Файл                | Результат                                                   |
|---------------------|-------------------------------------------------------------|
| data_large_512x.txt | Finished in 0.816535s, 1.2247 runs/s, 1.2247 assertions/s.  |
| data_large_256x.txt | Finished in 3.367310s, 0.2970 runs/s, 0.2970 assertions/s.  |
| data_large_128x.txt | Finished in 12.084400s, 0.0828 runs/s, 0.0828 assertions/s. |
| data_large_64x.txt  | Finished in 65.052992s, 0.0154 runs/s, 0.0154 assertions/s. |
| data_large_32x.txt  | Finished in 255.156939s, 0.0039 runs/s, 0.0039 assertions/s.|

Рассчитываем среднее отклонение увеличения времени выполнения программы при большом объеме данных:

| SEP                    | DEL                |
|------------------------|--------------------|
| 255.156939 / 65.052992 | 3.9222936740557603 |
| 65.052992 / 12.084400  | 5.383220681208831  |
| 12.084400 / 3.367310   | 3.5887399734506182 |
| 3.367310 / 0.816535    | 4.123901608626697  |
| AVG                    | 4.254538984335476  |

Рассчитываем время выполнения всех данных:
```ruby
0.816535 * (4.25 ** 10) => 1569871.5014487498
```

Рассчитываем на сколько нужно увеличить скорость обработки:
```ruby

"#{100.0 - (100 * 30.0 /  1569871.5014487498).round(3)}%" => "99.998%"
```

Рассчитываем эталонное время выполнения для файла `data_large_128x.txt`:
```ruby
"#{(12.084400 * 0.0019 / 100).round(5)}s" => "0.00023s"
```

Берем за основу для оптимизации файл `data_large_128x.txt`.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался `ruby-prof`, так же `rubocop-performance` для
обнаружения явных не эффективных участков кода. Для удобной отладки эффективности кода использовался `benchmark`.

### Находка №1
Первым что бросилось в глаза - огромное время выполнения метода `.select`.
Посмотрев в `101` строке логику выполнения метода `.select` стало понятно, что он сопоставляет пользователям их сессии,
при этом ему приходится перебирать весь список каждый раз. Такой поиск якно не эффективен и нуждается в изменении,
необходим быстрый, индексный поиск. Было на рассмотрении несколько вариантов - это с помощью *СУБД* и
штатныйми средствами *Ruby*. Поскольку применение *СУБД* усложнило бы программу и заняло бы дополнительное врямя был
выбран второй вариант. Для быстрого поиска был выбран поиск по `hash`-ключу.
В виде `hash`-ключа был выбран `ID` пользователя:
```ruby
{
  # ...
  '142' => {
    # User sessions data.
  }
  # ...
}
```

Изменение вида поиска дало существенный прирост производительности обработки более `60%`.

### Находка №2
При дальнейшем использовании `ruby-prof` было обнаружено, что много времени программа тратить на итерации в
вызовах метода `.each`. Было обнаружено, что парсинг данных происходит с помощью итератора `.each`.
Так как структкрв файла была в формате `csv` для более эффективного парсинга был выбран класс *CSV*.
Прирост от полученной производительности был не значителен, около `5-6%` от текущей.

### Находка №3
Следующую проблему, которую показал `ruby-prof` - преобразование каждой записи в обьект типа `hash`.
Отказ от преобразования данных дал прирост около`30%`. Основновным вопросом оставалось сохранить читабельность кода,
для этого номера колонок были вынесены в константы. Не большой прирост от полученной производительности (около `10%`)
было получено за счет упразнения преобразований, меморизации и количества итераций для составления отчета.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными: 

- С включенным `Garbage Collection` в среднем за `31sec`.
- С вылюченным `Garbage Collection` в среднем за `17sec`.

Удалось улучшить метрику системы с расчетных `1569871sec`, до `31sec` и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был переписан с
помощью инструмента `RSpec` тест на результат работы программы.
Дополнительно к тестирование был добавлен тест на проверку скорости выполнения в `30sec`.
