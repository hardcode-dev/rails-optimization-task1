# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: общее время выполнения программы.
Перед внесением изменений в программу было измерено время выполнения программы для тестовых файлов
Файл с 1_000 строками обработался за 0.08 секунды
Файл с 10_000 строками обработался за 4.5 секунды
Файл с 20_000 (784 КБ) строками обработался за 21 секунду
Файл сo 100_000 (4 МБ) строками
Файл с 3_250_940 (134 МБ) строками обработается как минимум за 1 час, а скорее всего еще больше.
Соотвественно можно сделать вывод, что с ростом размера файла растет и время обработки по более, чем линейной прогрессии.

Бюджет метрики - обработать полный файл минимум за 30 секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
- проверить время обработки данных
- настроить и провести профилирование
- модифицировать код
- проверить тесты
- замерить результаты выполнения

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- ruby-prof в режиме Flat
- ruby-prof в режиме Graph
- ruby-prof в режиме CallStack

Вот какие проблемы удалось найти и решить

### Находка №1
- Первое замерение ruby-prof в режиме Flat, время выполнения программы до оптимизации 4.5 секунды
- этот отчет показал долю 85.09 в общем времени выполнения программы на метод Array#select
- проблема метода: метод select выполняется над массивом sessions для каждого элемента массива users, соответственно, чем больше users и sessions, тем дольше будет выполняться этот метод
- гипотеза улучшения метода: 1 раз сгруппировать массив sessions на основе user_id, а затем брать заранее подготовленные данные
- результат - время выполнения программы снизилось до 0.67 секунды
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - метод select больше не присутствует в отчете
  - метод group_by используется 1 раз и занимает 0.005 секунды на выполнение (42 место среди методов)
  - основной точкой роста стал метод Array#each

### Находка №2
- При анализе двух отчетов моё внимание было направлено на метода split и map
- Второе замерение ruby-prof в режиме Flat и в режиме Graph, время выполнения программы до оптимизации 0.67 секунды

#### 2а
- проблема метода split № 1 - используется split при загрузке файла
- гипотеза улучшения метода split № 1 - использовать File.readlines без использования split
- результат - неизменный

#### 2б
- проблема метода split № 2 - используется split при обработке каждой строки, а затем дублируется при парсинге объекта user или session
- гипотеза улучшения метода split № 2 - передавать для парсинга уже разделенную строку
- результат - время обработки незначительно уменьшилось
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - количество вызовов метода split уменьшилось в 2 раза, он переместился со 2 на 4 позицию в отчете

#### 2в
- проблема метода map - использование цепочек из последовательных вызовов метода map, всего 16898 вызовов
- гипотеза улучшения метода map - преобразование цепочек в 1 использование метода, заменить map на inject, select, detect, где это возможно
- результат - время обработки уменьшилось
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - количество вызовов метода map уменьшилось до 6145 (почти в 3 раза), он переместился со 2 на 5 позицию в отчете

- результат - время выполнения программы снизилось до 0.62 секунды
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - методы map и split снизили время выполнения
  - основной точкой роста остается метод Array#each и Date#parse

### Находка №3
- При работе с программой заметил частое суммирование массивов для добавления новых элементов, что не есть оптимально
- Третье замерение ruby-prof в режиме Flat, время выполнения программы до оптимизации 0.62 секунды
- гипотеза улучшения - использовать unshift для добавления новых элементов в начало массива, исключение лишних переменных
- результат - время выполнения программы снизилось до 0.49 секунды
- тесты не показали ошибок

### Находка №4
- Четвертое замерение ruby-prof в режиме CallStack, время выполнения программы до оптимизации 0.49 секунды
- Основная точка роста - Array#each и дочерний метод Enumerable#all?, что указывает на сбор данных об уникальных браузерах
- гипотеза улучшения - использование map.uniq или проверка на any?, что не потребует проверять весь массив уникальных браузеров
- результат - время выполнения программы снизилось до 0.35 секунды, лучшее решение map.uniq, среднее - использование include? для проверки, худшее - any?
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - Основная точка роста - Object#collect_stats_from_users, занимает 70% времени

### Находка №5
- Пятое замерение ruby-prof в режиме CallStack, время выполнения программы до оптимизации 0.35 секунды
- Основная точка роста - Object#collect_stats_from_users
- проблема - методов вызывает 7 раз для всего массива users, и для каждого объекта user - проверяются все его sessions
- гипотеза улучшения - вызвать метод 1 раз и уже внутри него производить рассчеты
- результат - время выполнения программы снизилось до 0.19 секунды
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - Основная точка роста - парсинг даты в методе Object#collect_stats_from_users, занимает 53% времени
  - вторичная точка роста - чтение исходного файла с парсингом строк (25%)
  - генерация json (15%)

### Промежуточный результат
После всех изменений программы удалось добиться обработки полного файла за 86 секунд вместо более чем часа,
для файла с 20000 строками скорость выполнения программы увеличилась в 70 раз,
замеры показали, что относительный рост скорости тем больше, чем больше файл, т.е. для полного файла время обработки могло уменьшиться в 100 раз

Файл со 100_000 строками обрабатывается за 1.74 секунды

### Находка №6
- Шестое замерение ruby-prof в режиме CallStack, время выполнения программы до оптимизации 1.74 секунды
- Точка роста - генерация json
- гипотеза улучшения - использовать более оптимизированную библиотеку Oj
- результат - время выполнения программы снизилось до 1.57 секунды
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - генерация json занимает только 3% времени

### Находка №7
- Седьмое замерение ruby-prof в режиме CallStack, время выполнения программы до оптимизации 1.57 секунды
- Точка роста - парсинг даты
- проблема - парсинг даты занимает очень много времени
- гипотеза улучшения - анализ результата и исходных данных показал, что дата никак не преобразуется
- результат - время выполнения программы снизилось до 1.25 секунды
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - Основная точка роста Object#collect_stats_from_users, но время обработки занимает 41%
  - вторичная точка роста - парсинг входных данных - 39%

### Находка №8
- Время обработки полного набора данных примерно 58 секунд
- После подсказок в слаке провел оптимизацию, нужный массив данных формируется уже на этапе сбора данных, чтобы избежать повторных проходов через массивы
- проблема - прочитал что set упрощает работу с sort и uniq, но опытным путем установлено, что время только увеличивается
- результат - полное время выполнения программы снизилось до 35 секунд
- тесты не показали ошибок
- как изменился отчёт профилировщика:
  - Основная точка роста - парсинг входных данных, время обработки занимает 81%
    - а именно parse_session - 42% времени

### Итог
После последних изменений алгоритма время выполнения в среднем составляет 29 секунд
