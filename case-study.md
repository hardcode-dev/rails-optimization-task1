# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время обработки файла на 10 000 строк в секундах

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за несколько минут.

Вот как я построил `feedback_loop`:
- профилирование, поиск точки роста
- оптимизация bottleneck'а
- тестрование
- прогон benchmark
- если уложились в бюджет - коммитим изменения, иначе повторяем алгоритм

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался профилировщиком ruby-prof

Вот какие проблемы удалось найти и решить

### Находка №1
- ruby-prof в формате callstack показал, что первая точка роста - Array#select:
`user_sessions = sessions.select { |session| session['user_id'] == user['id'] }`
- группировка сессии по пользователям
- время обработки сократилось с 7,73 s до 1,03 s
- да. %self: 87% -> 0%

### Находка №2
- ruby-prof в формате flat показал, что следующая точка роста Array#all?
- получение списка уникальных браузеров через map и uniq:
`unique_browsers = sessions.map { |session| session['browser'] }.uniq`
- время выполнения сократилось с 1,03 s до 0,37 s
- да. %self: 26,1% -> 0,35%

### Находка №3
- ruby-prof в формате callstack -> `collect_stats_from_users`
- уменьшение числа вызовов метода, внутри метода сокращено количество вызовов map, по подсказке рубокопа конкатенация строк заменена на интерполяцию
- с 0,36 s до 0,29 s
- да

### Находка №4
- ruby-prof в формате flat -> Array#+
- заменила на `<<`
`users_objects = users.map { |user| User.new(attributes: user, sessions: sessions_by_users[user['id']] || []) }`
- c 0,29 s до 0,23 s
- да. %self: 19,24% -> 0%

### Находка №5
- ruby-prof в формате flat -> Array#each
- замена на map
- 0,2391 s -> 0,2214 s
- да

### Находка №6
- ruby-prof в формате flat -> String#split
- убран из методов `parse_user` и `parse_session`, сразу передается массив
- 0,2214 s -> 0,2109 s
- да. %self: 18,6% -> 9,86%

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы и уложиться в заданный бюджет. После проделанной работы обработка файла занимает ~40 s.

Обработка файла в 10 000 строк занимала ~7,72 с, после оптимизации время сократилось в 37 раз и стало ~0,21 с 

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были написаны performance-тесты, которые контролируют укладывается ли выполнение в заданный бюджет. Рассмотрены кейсы для файлов: 10 000, 20 000 и 40 000 строк, с бюджетом 100, 200 и 400 мс соответственно.

