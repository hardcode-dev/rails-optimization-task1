# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время выполнения на файле уменьшенного размера

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 1-2 секунды

Вот как я построил `feedback_loop`: 
- выключил GC 
- Создал набор тестовых файлов
- Нашел Главную Точку Роста
- Исправил ее
- Проверил тест

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался несколькими профилировщиками.

Вот какие проблемы удалось найти и решить

### Точка роста №1 
Для того, чтобы найти "точку роста" в этом шаге я воспользовался ruby-prof (wall_time flat). Использовался файл объемом ~0.5 mb тестовых данных. 

Тест указал, что главная точка роста -- это частое использование `Array#select`. Время, которое уходит на этот участок занимает `94.62%` от всего процессорного времени расчета.
К тому же используется он в этом месте не по назначению. 

- Общее время выполнения программы (`14.038075`)
- Время потраченое в точке роста ( `12.906` )

Для того что бы улучшить работоспособность, логика сбора статистики по пользователям была перенесена в цикл обработки данных при чтении из тестового файла.

После внесенных изменений общее время выполнения программы стало `0.740964`

### Точка роста №2
Для того, чтобы найти "точку роста" в этом шаге я воспользовался ruby-prof (wall_time graph). Использовался файл объемом ~5 mb тестовых данных. 

Тест указал, что главная точка роста -- это частое использование `Array#each`.
Время выполнения на данном этапе составило: `~8 sec`. 
Количество вызовов: `173064`

Шаги которые были проделаны на этом этапе. 
- Вынес логику сбора статистики в отдельный сервис. 
- Вынес сбор статистики для каждлого юзера в модель юзера. 
- Провел рефакторинг кода ответственного за сбор статистики
- сократил кол-во вызовов `Array#each`
- сократил кол-во вызовов `Array#map`
- сократил время выполнения почти в `2 раза`

### Точка роста №3

Для того, чтобы найти "точку роста" в этом шаге я воспользовался ruby-prof (wall_time flat). Использовался файл объемом ~5 mb тестовых данных. 

Тест указал, что главная точка роста: 
- частое использование `Array#each?`. (`calls`: 173064, `self_time`: 6.68%, `total`: 4.968 ) 
- частое использование `String#split`. (`calls`: 300000, `self_time`: 6.46%, `total`: 0.364 )

Время выполнения на данном этапе составило: `~7.5 sec`. 

Шаги которые были проделаны на этом этапе. 
- Избавился от ненужных вызовов `String#split`
- Изменил чтение из файла 
- Рефакторинг логики подсчета статистики по браузерам
- Заменил ключи в Хешах на `symbol`
- сократил время до `~2.5 sec`

### Точка роста №4

Для того, чтобы найти "точку роста" в этом шаге я воспользовался stackprof (CallGrind). Использовался файл объемом ~25 mb тестовых данных. 

Общее время выполнение программы оказалось `~13 sec`.
Тест указал, что главная точка роста - сбор статистики по пользователям (`wall_time`: ~ 5 sec).

Шаги которые были проделаны на этом этапе:
- Добавил классы для работы с `sessions`
- Вынес и отрефакторил логику, для формирования отчета в отдельный сервис. 
- Заменил используемый гем для работы с `JSON` объектами 
- Сократил время выполнения до `~10 sec` на файле объемом `~25 mb`

Выполнил программу на продакшн файле 6 раз с выключенным GC и отключенными тестами. 
Полученный результат: `~26.15 sec` ( http://joxi.ru/v29GXj8Tz41WOr )
 
### Точка роста №5

На самом деле это не совсем точка роста. Я просто решил немного отрефакторить текущий код и понял, что моя программа несколько раз бегает по всем данным.  
Я решил попробовать это исправить. 

- Избавился от лишних классов
- Добавил один класс, который отвечает за сбор статистики по мере чтения из файла 
- Вынес логику формирования отчета в отдельный сервис. 
- Внедрил использование `Set` и `SortedSet`   
- Сократил время выполнения до `~22 sec` на продакшн файле 
- Сократил время выполнения с включенным `GC` до `~28 sec` на продакшн файле 

[Скриншот с результатами:](http://joxi.ru/p274bMyTWZWkOA)

 
## Результаты

В результате проделанной оптимизации наконец удалось обработать файл с данными. 
Удалось улучшить метрику системы с того, что у вас было в начале, до `~22 sec` с выключенным GC и `~28 sec` с включенным GC и уложиться в заданный бюджет `30 sec`.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для того что бы защитить выполнение от регресии был добавлен тест на выполнение, который проверяет, что файл объемом `~1mb` обрабатывается меньше чем за 0.5 секунды.