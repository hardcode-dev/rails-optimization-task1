# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: время выполнения программы

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений менее чем за 10 секунд.

Вот как я построил `feedback_loop`: я написал рэйк-таску которая
1. прогоняет тест корректности работы
2. прогоняет тест произовдительности
3. собирает и выводит отчет ruby-prof
4. собирает и выводит отчет stackprof
5. замеряет и выводит метрику -- время выполнения программы на заданном примере

Для начального этапа оптимизации я из исходного файла создал файл размером 10000 строк

Каждую итерацию оптимизации я запускал таску и
- смотрел что прошли тесты корректной работы и производительности
- смотрел, изменились ли основные метрики и поменялась ли точка роста
- если первые два пункта выполнены то обновлял тест производительности и искал новые точки роста

## Вникаем в детали системы, чтобы найти главные точки роста
Для того чтобы измрерить асимптотикой я написал pending тест на производительность с ожиданием линейной сложности, который сообщил мне что текущая сложность O(N^2)
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof и stackprof, а также отчетами rubocop-perfomance и fasterer

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- ruby-prof показал что 87% врмени занимает Array#select который встречается в коде единожды: ser_sessions = sessions.select { |session| session['user_id'] == user['id'] }
- для оптимизации я сгруппировал сессии по user_id и заодно вынес код в отдельный метод collect_user_objects 
- метрика изменилась c 1.3с на 0.2с -- рост производительности в 6.5 раз
- согласно отчетам профайлера указанный метод перестал быть основной точкой роста, обновляю спеку и ищу новые точки роста

### Ваша находка №2
- всё тот же ruby-prof показал что Array#all? на данный момент находится в топе потребления, этот метод вызывается для подсчета уникальных браузеров
- так как далее уже есть код который собирает браузеры из сессий -- будем переиспользовать его
- новый рещультат метрики 0.15с вместо 0.2с
- согласно отчетам профайлера указанный метод перестал быть основной точкой роста, обновляю спеку и ищу новые точки роста

### Ваша находка №3
- на текущем этапе я обратил внимание что сложность все еще не линейная, чтобы поискать главную точку роста я увеличил количество строк в файле до 100_000. Текущее значение метрики 3.8с. ruby-prof показывает что Array#each занимает 68% времени и является главной точкой роста
- для начала самое просто -- заменим сложение массивов записью в массив и уберем лишние join-ы
- значение метрики изменилось с 3.8с на 0.88с
- уже лучше, но сложность все еще не линейная и Array#each всё еще в топе потребления с 21%

### Ваша находка №4
- я заметил что в отчете stackprof топ занимает метод collect_stats_from_users, а руби-проф flat репорт стал показывать в сумме сильно меньше 100%, заменил его на callstack и увидел что в топе на самом деле метод collect_stats_from_users с 72%. Заодно я снова увеличил размер файла до 500_000 строк, метрика показывает 5.8с
- для удобства дальнейшего рефакторинга упрощаю метод collect_stats_from_users так чтобы он собирал все данные за раз, также убираю лишние манипуляции с данными и лишний парсинг даты
- значение метрики изменилось с 5.8с на 2.16с
- согласно отчетам профайлера указанный метод перестал быть основной точкой роста, можно проверять на основном наборе данных


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 1.3с до 0.03с на наборе в 10000 строк
Удалось обработать файл data_large.txt за 23 секунды и уложиться в заданный бюджет, таким образом дальнейшие оптимизации не требуются
Допольнительно обнаружил, что после проделанных оптимизаций отчеты rubocop-perfomance и fasterer не дают рекомендаций

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
