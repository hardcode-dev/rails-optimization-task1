# Case-study оптимизации
## Актуальная проблема
Необходимо было обработать файл с данными, чуть больше ста тридцати мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.
Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *скорость обработки файла*

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался 

* benchmark

* ruby-prof

Вот какие проблемы удалось найти и решить

### Находка №1
Чтобы выровнять разброс при запуске, я отключал GC и запускал обработку файла 1000 раз для каждой итерации
замеры для разного колличества пользователей

01: 0.00018597

03: 0.00029654

04: 0.00041578

08: 0.00069441

16: 0.00184864

24: 0.00355275

32: 0.00582529

график не линейный и при увеличении пользователей скорость обработки будет будет стремительно падать

### Находка №2
- RubyProf::FlatPrinter показал, что 17.71% приходится на <Class::Date>#parse

измени с

user.sessions.map{|s| s['date']}.map {|d| Date.parse(d)}.sort.reverse.map { |d| d.iso8601 }

на 

user.sessions.map{|s| s['date']}.sort.reverse

тайминг изменился
0.00029654 -> 0.00020452 

первая строка 22.12% <Class::Date>#parse 
 
изменилась на 17.86%  <Class::IO>#write

RubyProf::CallStackPrinter выдает

89.66% (89.85%) <Class::IO>#write [1 calls, 1 total]

### Находка №3
Обнаружил, что 95% (6.5сек)выполнялось sessions.select,
избавился от это путем формиморования хешей.
строка ушла из отчета а время выполнения стало 0.28сек

### Находка №4
57.80% Array#each

обнаружил, что чтение из файла в массив не эффективно, переделал

### Находка №5
общее время выполнения большого файла 70.07сек

48.68% (48.68%) Object#collect_stats_from_users [7 calls, 7 total]

переделал collect_stats_from_users, исключив лишнее присвоение 55.93сек

подчистил map 53.07527449

отчет выдал такое
39.67% (39.67%) Array#each [2 calls, 9 total]
37.39% (100.00%) Array#each [7 calls, 9 total]

### Находка №6

увидел, что какое-то время тратится на приведение в сессии, оптимизировал

46.46сек

### Находка №7

24.73% времени тратится на Object#parse_session

33.32% (33.32%) Object#collect_stats_from_users

эти две проблемы остались, но далее пока не вижу как их исправить

### Находка №8 

Дошло, что 2 цикла которые жрут больше всего можно объединить в один

отрефакторил модель User и объединил циклы

40.43 сек

### Находка №9

14.78%   String#split

переключил на парсинг строки единожды

выхлопа слабый, на грани статистической погрешности

14.58% JSON::Ext::Generator::GeneratorMethods::Hash#to_json

заменил гем json

36.369 cек

### Находка №10

8.96% user.sessions.map { |s| s['time'] }.sum.to_s + ' min.'

я применил метод reduce вместо map

35.07 сек, изменения на уровне статистической погрешности

### Находка №11

это не находка а здравый смысл, никакой инструмент в этом не помог. объединил все действия с пользователем в один прогон.

Finish in 29.99357505
