# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать
такую метрику:
- время выполнения программы в секундах

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы
при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`,
который позволил мне получать обратную связь по эффективности сделанных изменений за ~10 секунд

Вот как я построил `feedback_loop`:
- измерил время выполнения программы с входными данные размером в 1000 строк через Benchmark.ips, выразил время выполнения в секундах через 1/ips
- написал performance-тест выполнения за это время
- добавил еще несколько файлов входных данных разных размеров, написал тест асимптотики, получилась степенная зависимость
- далее использовал эти же входные данные для профилирования и сравнения метрик

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- rubyprof
- stackprof

rbspy не смог воспользоваться, так как не поддерживает ruby 2.7.3

Вот какие проблемы удалось найти и решить:

### Находка №1
- callgrind показал, что метод Array::all? вызывается 1000 раз, тратит 336615 (27%) времени, при этом в коде он встречается один раз.
- переписал подсчет уникальных браузеров через .uniq
- увеличение ips с 6.146 до 7.577
- число вызовов Array::all? сократилось до 154, время сократилось до 2313 (0.29%), общий wall_time cost так же сократился с 1217902 до 802154.

### Находка №2
- callgrind показал, что метод Array::select вызывается 154 раз, тратит 521405 (65%) времени, при этом в коде он встречается один раз.
- перенес парсинг статистики по пользователям в самое начало метода, разбив файл на блоки сессий по пользователям и
  сразу сохранив в объекты
- увеличение ips с 7.577 до 18.785. Асиптоматика в тестах приближается к линейной
- вызовов Array::select больше нет, общий wall_time cost же сократился с 802154 до 293426.

### Находка №3
- callgrind показал, что метод Array::map вызывается 1697 раз, тратит 74600 (25%) времени, при этом в коде он встречается 14 раз.
- последовательные вызовы map объединил в один
- увеличение ips с 18.785 до 20.062
- число вызовов Array::map сократилось до 1080, время сократилось до 53536 (20%), общий wall_time cost сократился с 293426 до 271151.

### Находка №4
- callgrind показал, что метод Date::parse вызывается 846 раз, тратит 31806 (12%) времени, при этом в коде он встречается 1 раз.
- убрал вызов этого метода, так как дата уже в формате iso8601
- увеличение ips с 20.062 до 30.194
- вызовов Date::parse больше нет, общий wall_time cost сократился с 271151 до 176691.

### Находка №5
- callgrind показал, что метод String::split вызывается 2155 раз, тратит 16517 (9%) времени, при этом в коде он встречается 5 раз.
- убрал лишние вызовы split из parse_user и parse_session
- увеличение ips с 30.194 до 35.468
- число вызовов Array::map сократилось до 1155, общий wall_time cost сократился с 176691 до 160173.

### Находка №6
- в callgrind методы Array::each и Array::map все еще занимают много времени: 58502 и 41785 соответственно
- объединил вычисление collect_stats_from_users в один хэш, вынес одинаковые map в переменные
- увеличение ips с 35.468 до 45.214, тест начинает через раз определять асимптоматику как линейную
- время Array::each и Array::map сократилось до 41028 и 18544, общий wall_time cost сократился с 160173 до 109090.

### Находка №7
- в callgrind метод Array::each занимает 41028 времени
- перенес collect_stats_from_users в цикл парсинга юзеров, удалил класс User
- увеличение ips с 45.214 до 53.369
- время Array::each сократилось до 25437, общий wall_time cost сократился с 109090 до 101087.

### Находка №8
- в callgrind метод Array::each занимает все еще много времени, по сравнению с остальными вызовами. data_large все еще
  очень медленно обрабатывается, возможно потребляется слишком много памяти при чтении всего файла.
- переписал на парсинг файла построчно, используя File.foreach, отчет сразу заполняется по ходу парсинга, кроме
  аггрегационных статистик в usersStats
- ips почти не изменился: 53.369 до 53.616. Однако сравнив метрику на большом файле в 16000 строк,
  результат оказался значительным: ips возрос с 2.251 до 3.116
- вызовов Array::each больше нет, вместо него стал занимать время IO::foreach, общий wall_time cost для файла в 16000 строк сократился c 2305374 до 1948966

## Результаты
- В результате проделанной оптимизации наконец удалось обработать большой файл в 3250940 строк за 30 секунд, а именно за 20.3281288 с. на i7-4770.
- Удалось улучшить метрику системы с 6.4208693 до 0.0795495 с. для файла в 16000 строк и уложиться в заданный бюджет.
- Использование гема OJ увеличило работу метода с data_large.txt на 2 секунды, в итоге оставил стандартный .to_json
- Преобразовал все ключи в хэшах из строковых в символьные, согласно fast-ruby такие ключи работают быстрее, в моем случае метрика почти не поменялась, но зато код выглядит чище.
- Отчеты rubocop-performance и fasterer после оптимизации - зеленые

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были написаны performance-тесты на
время выполнения и асимптотику с набором входных данных разного размера.

