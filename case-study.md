# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Предварительно были сформированы файлы обёмом от 1-5Mb, чтобы попробовать предугадать и сколько будет обрабатываться полный объём данных.

Были получены следующие результаты, до оптимизации:

```
Calculating -------------------------------------
           File: 1Mb      0.113  (± 0.0%) i/s -      1.000  in   8.840352s
           File: 2Mb      0.023  (± 0.0%) i/s -      1.000  in  42.861275s
           File: 3Mb      0.008  (± 0.0%) i/s -      1.000  in 132.722494s
           File: 4Mb      0.003  (± 0.0%) i/s -      1.000  in 295.557700s
           File: 5Mb      0.002  (± 0.0%) i/s -      1.000  in 507.270327s
                   with 99.0% confidence

Comparison:
           File: 1Mb:        0.1 i/s
           File: 2Mb:        0.0 i/s - 4.85x  (± 0.00) slower
           File: 3Mb:        0.0 i/s - 15.01x  (± 0.00) slower
           File: 4Mb:        0.0 i/s - 33.43x  (± 0.00) slower
           File: 5Mb:        0.0 i/s - 57.38x  (± 0.00) slower
                   with 99.0% confidence
```

Т.к. уже на объёмах в 5Mb мы видим слишком длительную работу стартового алгоритма, то было принято решение,
за целевую метрику взять скорость обработки файла объёмом 1Mb и установить для неё целевой показатель в 1 секунды.
Был написан тест, который показывает, что сейчас обработка 1Mb данных укладывается в 8 секунд.

```
1) Performance works under 1500ms
     Failure/Error:
       expect do
         work(input_filename: './profiling/files/data_1Mb')
       end.to perform_under(1500).ms.warmup(2).times.sample(10).times

       expected block to perform under 1.5 sec, but performed above 5.74 sec (± 93.1 ms)
```
При дальнейших оптимизациях мы можем с помощью этого теста проверять насколько близко мы подбираемся к цели.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений не более чем за 30 секунд.

Вот как я построил `feedback_loop`:
- Берём минимально допустимый объём данных, на которых можем запустить начальный алоритм.
- Выделяем базовую метрику, время.
- Добавляем тест на ререссию по времени
- Профилирование
- Рефакторинг и оптимизация кода
- Запуск тестов, сбор метрик
- Увеличиваем объём данных x2

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- gem 'benchmark-ips'
- gem 'rspec-benchmark'
- gem 'ruby-prof'

Вот какие проблемы удалось найти и решить:

### Находка №1

После прогона ruby-prof, было обнаружено, следующее:

```
Measure Mode: wall_time
Thread ID: 580
Fiber ID: 560
Total: 39.225927
Sort by: self_time

 %self      total      self      wait     child     calls  name                           location
 94.92     37.235    37.233     0.000     0.003     4099   Array#select
  2.18      0.857     0.854     0.000     0.003    26727   Array#all?
  1.11     39.131     0.436     0.000    38.694      174  *Array#each
  0.50      0.419     0.195     0.000     0.225    44889   Array#map
  0.24      0.174     0.093     0.000     0.082    22614   <Class::Date>#parse
  0.16      0.063     0.063     0.000     0.000    53409   String#split
  0.10      0.064     0.041     0.000     0.024    22614   Object#parse_session
```
- Видим большое кол-во вызовыов Date#parse хотя в исходном файле дата уже в нужном формате.
- Видим большое кол-во вызывывов split.
- Поле age, session_id никак не используется в отчётах, поэтому не будем собирать эти данные.
- Вместо отдельных полей для имени пользователя, делаем одно поле fullname.

Попробуем убрать использование Date.parse + использование split в parse_user и parse_session.

Получаем следующий результат:

```
 0.16      0.062     0.062     0.000     0.000    26693   String#split
```
- Вызов split уменьшился в 2 раза, сопостовимо с кол-вом строк в файле.
- Вызовов Date#parse не замечено

### Находка №2

Вместо выгрузки файла полностью в память, заеняем `File.open` на `IO.foreach`. Что позволяет нам экономить на памяти.
Этот рефакторинг больше в сторону оптимизации памяти, чем по CPU.

### Находка №3

Вместо стандартной библиотеки json используем oj, это опять же улучшение в сторону работы с памятью, но всё же это даёт небольшой буст в обработке данных.

После всех оптимиация выше, получаем следующий результат:

```
Calculating -------------------------------------
           File: 1Mb      0.128  (± 0.0%) i/s -      1.000  in   7.836691s
           File: 2Mb      0.025  (± 0.0%) i/s -      1.000  in  40.232757s
           File: 3Mb      0.008  (± 0.0%) i/s -      1.000  in 126.115113s
           File: 4Mb      0.004  (± 0.0%) i/s -      1.000  in 269.945373s
           File: 5Mb      0.002  (± 0.0%) i/s -      1.000  in 439.858670s
                   with 99.0% confidence
```
### Находка №4

```
94.92     37.235    37.233     0.000     0.003     4099   Array#select
```
Самые большие затраты при сборе данныз посессия пользователя, здесь не обойтись без рефакторинга кода и оптимизации процесса сбора данных.
После отимизации агрегации данных пользователей, получаем следующий результат.

```
Comparison:
           File: 1Mb:       23.7 i/s
           File: 2Mb:       11.9 i/s - 2.00x  (± 0.07) slower
           File: 3Mb:        8.0 i/s - 2.98x  (± 0.10) slower
           File: 4Mb:        6.0 i/s - 3.96x  (± 0.16) slower
           File: 5Mb:        4.7 i/s - 5.03x  (± 0.19) slower
                   with 99.0% confidence
```

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 8 сек обработки файла с данными объёмом 1MB до обработки этого же файла за 50ms.
Благодаря всем оптимизациям получилось обработать файл data_large.txt в среднем за 9.68 sec (при подсчёте использовался rspec-benchmark).

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы был написан тест с использованием rspec-benchmark, чтобы замерять время при обработке минимально допустимого объёма данных. Если перформанс этого теста будет показывать деградацию, значит мы что-то делаем не так.
