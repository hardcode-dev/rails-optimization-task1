# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
1) Подготовил тестовые данные для постепенной оптимизации (2k, 10к, 50к, 100к)
2) Расставил прогресс бары для более наглядной работы скрипта
3) Написал бенчмарк скрипт позволяющий сравнивать скорость выполнения и обьем данных
4) Написал несколько профайлеров позволяющих наглядно искать точку роста

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 2,5 минуты

Вот как я построил `feedback_loop`:
1) Сгенерировал отчеты ruby-prof для поиска проблем
2) Определил точку роста
3) Рефакторинг проблемного места
4) Повторная проверка при помощи бенчмарк теста и составления обновленных отчетов ruby-prof

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался
Отчеты, прргресс бары и тест.

Вот какие проблемы удалось найти и решить

### Заход №1 Выборка пользовательских сессий
- Занимает основное время программы (выявлено через прогресс бар и отчеты ruby-proof)
- Попробовал с ходу просто поиграть с вариантами выборки (filter, grep) а так же уменьшать искомый массив для каждой итерации
- Выиграл 30 секунд от 2,5 минут.
- Отчет особо не поменялся, главный актор просто сменил название, выигрыш не особо значителен

### Заход №2 Выборка пользовательских сессий
- Занимает основное время программы (выявлено через прогресс бар и отчеты ruby-proof)
- Подумал и решил сразу парсить строку куда надо. Для этого отрефачил метод work. Идея - не пытаться обработать все сразу а брать только текущую строку и уже с ней взаимодействовать.
- Переместился на следующий горизонт - 1,5 миллиона записей за 63 секунды (до этого было 50к за 2,5 минуты!)
- Отчет показал что теперь главная точка роста - each главный итератор, и сразу несколько точек в нем с равной ценностью

### Заход №3 Неоптимальные sort! и uniq!
- Выявлены на предыдущем шаге через профайлер, кушают много времени
- Отредактировал.
- Горизонт 1,5 миллиона записей начал проходить тест, data_large наконец то стал завершаться с результатом доступным для дебага но еще не оптимальным для бюджета
- Отчет показал что теперь главная точка роста - map при форматировании дат.

### Заход №4 map в датах.
- Выявлены на предыдущем шаге через профайлер, кушает много времени
- Отредактировал, убрал лишнее.
- Вписался в бюджет large_txt
- Отчет показал что теперь одно из самых жирных мест - обработка сессии а так же выявление булевых состояний

### Заход №5 Булевы поля и обработка сессии
- Иногда вываливаемся из бюджета на пару секунд, поэтому решил копать дальше. Отчеты указывали на долгую обработку сессий а так же относительно долгую запись в json. Плюс итерирование всегда занимает большую часть времени.
- Попробовал использоваь csv (foreach), как оказалось работать с ним удобнее но он медленее (+15 секунд!). Вернул на место. Оптимизировал булевы методы для флагов, отрефачил сборку бразуеров убирая лишние телодвижения. Для сборки json добавил oj, прошелся rubocop perfomance. Включил ноутбук в розетку.
- Уже уверенно вписываюсь в бюджет.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с того что вообще не заканчивалось и уложиться в заданный бюджет (от 25 до 29 секунд).
Теоритически можно еще покопать в сортировки, думаю где то есть что то быстрее, а так же можно попробовать поиграть с регексами.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал benchmark_spec.rb


