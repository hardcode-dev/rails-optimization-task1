# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: сократить файл data_large.txt до 10_000 строк и проверить скорость работы программы, затем постепенно увеличивать кол-во строк с шагом Х2.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 3 сек, с файлом на 40_000 строк, вместо 41 до изменений

Вот как я построил `feedback_loop`: создал несколько файлов на 10_000, 20_000, 40_000 строк и начал их тестировать.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался RubyProf GraphHtmlPrinter

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- большое время занимало выполнение метода `.select` в `generate_users_objects`
- было решено заменить этот метод на хэш с ключом "user_#{user['id']}", что значительно сократило время выполнения программы

### Ваша находка №2
- отчет `StackProf` показал, что теперь наибольшее время занимает подсчет уникальных браузеров, что и стало новой точкой роста
- вместо перебора сессий методом `each` и сравнением каждого значения я решил воспользоваться методом `uniq` для массива всех сессий и подсчитать их количество методом `count`
- переписал способ подсчета уникальных браузеров
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №3
- далее в отчете было несколько мест в кортые занимали не слишком большое кол-во времени, но все-таки ощутимое
- и я решил заняться небольшим рефакторингом

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с того, что у файл `data_large.txt` вообще не мог обработаться и занимал 100% CPU, судя по данным rbspy, до того, что файл с отключенным GC стал обрабатываться за 35 сек и с "натяжкой" уложиться в заданный бюджет.

В целом еще есть над чем поработать и что оптимизировать. Но в данном случае счел данный результат достаточным, т.к. и так немного отстал от курса.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы использовался тест.