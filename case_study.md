# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: общее время выполнения программы (считаем с помощью Benchmark.realtime).

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 30 секунд.

Вот как я построил `feedback_loop`:

- Создал отдельные руби-файлы для бенчмарка, профилировщиков, тестов.
- Создал файлы с исходными данными с меньшим количеством строк: 256, 512, 1024 и тд. для бенчмарка и профилирования. По мере продвижения в оптимизации увеличивал количество строк. Критерием было, чтобы процессинг файла шел меньше 10 секунд.
- После внесенных изменений в код, делал следующее:
- 1) Проверял, что тест проходит.
- 2) Запускал бенчмарк.
- 3) Запускал интересующие меня профилировщики и изучал отчеты.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof, stackprof, rbspy. Больше пользовался ruby-prof с принтерами graph и callstack и rbspy.

Вот какие проблемы удалось найти и решить

### Итерация 1:

Выстроил feedback-loop. Попробовал все виды профилировщиков. Увидел, что большую часть времени процесс проводит в Array#select. Вынес sessions.select в отдельный метод, чтобы локализовать проблему:

def select_sessions_for_user(sessions, user)
  sessions.select { |session| session['user_id'] == user['id'] }
end

Запустил ruby-prof с принтером graph и убедился, что проблема с Array#select именно в этом месте.
Это и есть главная точка роста.

Как решил оптимизировать:
Использовать хэш для хранения сессий и поиска их по id пользователя.

Как изменилась метрика:
На данных с 16384 строками время выполнения уменьшилось с ~10 секунд до 0.8.

Как изменился отчёт профилировщика:
Исправленная проблема перестала быть главной точкой роста.

### Итерация 2:

По отчетам профилировщика ruby-prof с принтерами graph и callstack понял, что следующая точка роста - вызов Enumerable#all?.

Вынес подсчёт количества уникальных браузеров в отдельный метод, и убедился, что большое количество времени тратится именно в этом месте.

Как решил оптимизировать:
Убрать проверку всего массива браузеров на уникальность нового элемента. Убрать лишний проход по массиву сессий и считать уникальные браузеры во время итерации по строкам файла.

Как изменилась метрика:
На данных с 65536 строками время выполнения уменьшилось с ~6.24 сек до ~5.45 сек.

Как изменился отчёт профилировщика:
Проблема исчезла из отчета, стала очевидна следущая главная точка роста.

### Итерация 3:

По отчетам профилировщика ruby-prof с принтером callstack понял, что следующая точка роста - вызов Date.parse.
Вынес сбор дат сессий в отдельный метод.

Как решил оптимизировать:
Убрать лишний код: Date.parse и приведение к iso8601 в данном случае не имеют смысла, так как исходные данные и так в правильном формате.

Как изменилась метрика:
Удивительно, что метрика изменилась незначительно.

Как изменился отчёт профилировщика:
Метод, в котором происходит сбор дат сессий, стал занимать незначительно время в отчете.

### Итерация 4:

Профилировщики показали, что проблема с Array#each. Однако, такой ответ меня не устроил, слишком размыто. Я решил вынести части кода в отдельные методы, чтобы найти главную точку роста.

Сделав это, понял, что основная проблема при чтении файла и итерации по его строкам.

Заменил 'File.read(...).split("\n")' на чтение файла по строкам (File.foreach(...)).

Как изменилась метрика:
На данных с 65536 строками время выполнения уменьшилось с ~5.3 сек. до ~4.8 сек.

Как изменился отчёт профилировщика:
Профилировщик продолжал указывать, что проблема в методе read_file (метод, в который я вынес чтение файла и итерацию по его строкам).

### Итерация 5:

Я не очень понял, как я должен был точно это увидеть из отчета профилировщика; но так как я максимально локализовал медленный код, и других вариантов не было, то стало понятно, что производительность страдает из-за конкатенации массивов, которая происходит при чтении файла.

Решил это место переписать c "arr += [elem]" на "arr << elem".

Как изменилась метрика:
Значительно улучшилась. На данных с 65536 строками время выполнения уменьшилось с ~4.8 сек. до ~1.3 сек.

Как изменился отчёт профилировщика:
Метод read_file продолжал занимать значительное время выполнения (26%), но более не являлся главной точкой роста.

### Итерация 6:

Теперь главной точкой роста являлся Array#each, вызываемый из Object#collect_stats_from_users.
Там было много лишних проходов по массивам users и sessions, я решил в первую очередь попробовать собирать все данные за один проход.

Как изменилась метрика:
На данных с 65536 строками время выполнения уменьшилось с ~1.3 сек до ~1 сек.

Как изменился отчёт профилировщика:
Object#collect_stats_from_users продолжал занимать значительное время выполнения (23%), но более не являлся главной точкой роста.

### Итерация 7:

По отчету профилировщика нашел еще одно место, в котором по аналогии с итерацией 5 нужно было переписать "arr += [elem]" на "arr << elem".

Как изменилась метрика:
На данных с 65536 строками время выполнения уменьшилось с ~1 сек до ~0.67 сек.
На данных с 524288 строками время выполнения уменьшилось еще более значительно: с ~20 сек до ~5.7 сек.

Как изменился отчёт профилировщика:
Изменилась главная точка роста.

### Итерация 8:

Главной точкой роста теперь являлся JSON::Ext::Generator::GeneratorMethods::Hash#to_json.

Переписал, используя оптимизированную библиотку "oj".

Как изменилась метрика:
На данных с 524288 строками время выполнения уменьшилось c ~5.7 сек до ~5.2 сек.

Как изменился отчёт профилировщика:
Изменилась главная точка роста.

### Итерация 9:

Я заметил, что в отчете профилировщика несколько раз повторялось String#split с процентов времени около 9-10%, решил, что в сумме это может быть достаточно много.

На самом деле нам нужно сделать только один String#split на каждую строку исходного файла, убрал лишние вызовы.

Как изменилась метрика:
На данных с 524288 строками время выполнения уменьшилось c ~5.2 сек до ~5.1 сек.
Очевидно, что это не являлось главной точкой роста.

Как изменился отчёт профилировщика:
Ничего особо не поменялось.

### Итерация 10:

Заметил, что в отчете профилировщика указывалось на проблемы с Array#each и с методом Object#parse_session_line, в который я вынес парсинг строки исходного файла с данными сессии.

Заметил, что я создаю лишний огромный массив сессий, решил это переписать.

Как изменилась метрика:
На данных с 524288 строками время выполнения уменьшилось c ~5.1 сек до ~4.1 сек.

Как изменился отчёт профилировщика:
Что-то поменялось, но по-прежнему есть проблемы с Object#parse_session_line.

### Итерация 11:

Профилировщик продолжал указывать на Array#each, вызываемый из Object#collect_stats_from_users. Я понял, что код пробегает по огромному массиву несколько раз. Переписал это.

Как изменилась метрика:
На данных с 524288 строками время выполнения уменьшилось c ~4.1 сек до ~3.9 сек.

Как изменился отчёт профилировщика:
Профилировщик продолжал указывать, что Array#each, вызываемый из Object#collect_stats, занимает 45% времени выполнения. Остальную половину времени занимало чтение из файла с итерацией по его строкам.

### Итерация 12:

После последней интерации профилировщик продолжал указывать, что Array#each, вызываемый из Object#collect_stats, занимает 45% времени выполнения. Остальную половину времени занимало чтение из файла с итерацией по его строкам.

Так как идей уже особо не было, я решил просто вычистить лишние итерации по массивам, вызовы функций и тд, это не заняло много времени.

Как изменилась метрика:
22 секунды для файла data_large.txt при бюджете 30 сек.

Как изменился отчёт профилировщика:
Картина изменилась, образовался перекос времени выполнения в сторону <Class::IO>#foreach. Думаю, возможно еще улучшить производительность, но так как метрика стала укладываться в бюджет, я решил остановиться на этом.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал простой performance-тест с использованием rspec-benchmark.


